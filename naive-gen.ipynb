{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained('bert-large-uncased')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shields is the capital of south korea . => is\n",
      "seoul ##amp the capital of south korea . => is\n",
      "seoul is ##6 capital of south korea . => the\n",
      "seoul is the tack of south korea . => tack\n",
      "seoul is the capital urgency south korea . => of\n",
      "seoul is the capital of 900 korea . => 900\n",
      "seoul is the capital of south ##×¨ . => .\n",
      "seoul is the capital of south korea gilded => .\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "original_sent = 'seoul is the capital of south korea .'.lower().split()\n",
    "\n",
    "for ii in range(len(original_sent)):\n",
    "    new_sent = copy.copy(original_sent)\n",
    "    new_sent[ii] = '[MASK]'\n",
    "#     new_sent[ii] = tokenizer.convert_ids_to_tokens([numpy.random.randint(0, len(tokenizer.vocab))])[0]\n",
    "    out = model(torch.tensor([tokenizer.convert_tokens_to_ids(new_sent)]))\n",
    "    pred = tokenizer.convert_ids_to_tokens([out[0][ii].max(0)[1].item()])[0]\n",
    "    print(\" \".join(new_sent), \"=>\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['way']"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1996, 3574, 1997, 2166, 2003, 999, 20703, 1997, 2037, 2279, 2299, 2108, 1012, 1000, 2748, 1012, 2028, 3634, 2809, 4551, 1000, 1011, 2027, 2024, 2315]\n",
      "the meaning of life is ! refrain of their next song being . \" yes . one hundred eight billion \" - they are named\n",
      "the meaning of life is ! refrain of their next song being . \" yes . one hundred eight billion \" - they are named\n"
     ]
    }
   ],
   "source": [
    "''' sequential generation '''\n",
    "\n",
    "sample = True\n",
    "max_len = 20\n",
    "leed_out_len = 3 #max_len\n",
    "random_future = False\n",
    "\n",
    "seed_text = 'the meaning of life is'.split()\n",
    "seed_len = len(seed_text)\n",
    "\n",
    "init_text = seed_text + ['[MASK]'] * max_len\n",
    "init_idx = tokenizer.convert_tokens_to_ids(init_text)\n",
    "if random_future:\n",
    "    for ii in range(max_len):\n",
    "        init_idx[seed_len+ii] = numpy.random.randint(0, len(tokenizer.vocab))\n",
    "\n",
    "for ii in range(max_len):\n",
    "    out = model(torch.tensor([init_idx[:seed_len+ii+leed_out_len]]))\n",
    "    if sample:\n",
    "        dist = torch.distributions.categorical.Categorical(logits=out[0,seed_len+ii])\n",
    "        init_idx[seed_len+ii] = dist.sample().item()\n",
    "    else:\n",
    "        init_idx[seed_len+ii] = torch.max(out[0, seed_len+ii],0)[1].item()\n",
    "\n",
    "print(init_idx)\n",
    "print(\" \".join(tokenizer.convert_ids_to_tokens(init_idx)))\n",
    "print(\" \".join(tokenizer.convert_ids_to_tokens(init_idx)).replace(\" ##\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1 the meaning of life is a turn 30 percent male milo children for played with 43 \" celebrated \" \" makes ##hop matching \" said\n",
      "iter 11 the meaning of life is that and i and . . to as i , i of that \" to and i i \" ,\n",
      "iter 21 the meaning of life is that and i and . . to an i , i of that \" to and i i \" ,\n",
      "iter 31 the meaning of life is of and i . . . to an i , i of that \" to and and i \" ,\n",
      "iter 41 the meaning of life is of and i . . . to an i , i of that \" to and and i \" ,\n",
      "iter 51 the meaning of life is of and i . . . to an i , i of that \" to and and that in ,\n",
      "iter 61 the meaning of life is of , who . . . to of \" , i of those , , and and that , ,\n",
      "iter 71 the meaning of life is to , to . . . to i \" , and , and , , and and that , ,\n",
      "iter 81 the meaning of life is to , to . . . to i \" , and , and , and and and they were ,\n",
      "iter 91 the meaning of life is to , and . . . , and , , and , and , and , . . . ,\n"
     ]
    }
   ],
   "source": [
    "''' parallel generation '''\n",
    "\n",
    "sample = True\n",
    "max_iter = 100\n",
    "viz_int = 10\n",
    "max_len = 20\n",
    "\n",
    "seed_text = 'the meaning of life is'.split()\n",
    "seed_len = len(seed_text)\n",
    "\n",
    "init_text = seed_text + ['[MASK]'] * max_len\n",
    "init_idx = tokenizer.convert_tokens_to_ids(init_text)\n",
    "for ii in range(max_len):\n",
    "    init_idx[seed_len+ii] = numpy.random.randint(0, len(tokenizer.vocab))\n",
    "\n",
    "for ii in range(max_iter):\n",
    "    out = model(torch.tensor([init_idx]))\n",
    "    for kk in range(max_len):\n",
    "        if sample:\n",
    "            dist = torch.distributions.categorical.Categorical(logits=out[0,seed_len+kk])\n",
    "            init_idx[seed_len+kk] = dist.sample().item()\n",
    "        else:\n",
    "            init_idx[seed_len+kk] = torch.max(out[0, seed_len+kk],0)[1].item()\n",
    "    if numpy.mod(ii, viz_int) == 0:\n",
    "        print(\"iter\", ii+1, \" \".join(tokenizer.convert_ids_to_tokens(init_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1 the meaning of life is layla (*) [MASK] [MASK] [MASK] [MASK]\n",
      "iter 11 the meaning of life is . true - [MASK] was (*)\n",
      "iter 21 the meaning of life is not corrupted is (*) from the\n",
      "iter 31 the meaning of life is disco (*) played were not it\n",
      "iter 41 the meaning of life is ku the (*) and find cholera\n",
      "iter 51 the meaning of life is in english (*) , f ##gr\n",
      "iter 61 the meaning of life is both (*) the , and the\n",
      "iter 71 the meaning of life is both the , and (*) the\n",
      "iter 81 the meaning of life is about the man (*) and the\n",
      "iter 91 the meaning of life is about the man and (*) the\n"
     ]
    }
   ],
   "source": [
    "''' parallel-sequential generation '''\n",
    "\n",
    "# sample = True\n",
    "burnin = 50\n",
    "max_iter = 100\n",
    "viz_int = 10\n",
    "max_len = 5\n",
    "\n",
    "seed_text = 'the meaning of life is'.split()\n",
    "seed_len = len(seed_text)\n",
    "\n",
    "init_text = seed_text + ['[MASK]'] * max_len\n",
    "init_idx = tokenizer.convert_tokens_to_ids(init_text)\n",
    "# for ii in range(max_len):\n",
    "#     init_idx[seed_len+ii] = numpy.random.randint(0, len(tokenizer.vocab))\n",
    "\n",
    "for ii in range(max_iter):\n",
    "    kk = numpy.random.randint(0, max_len)\n",
    "    init_idx[seed_len+kk] = tokenizer.convert_tokens_to_ids(['[MASK]'])[0]\n",
    "    out = model(torch.tensor([init_idx]))\n",
    "    if ii < burnin:\n",
    "        dist = torch.distributions.categorical.Categorical(logits=out[0,seed_len+kk])\n",
    "        init_idx[seed_len+kk] = dist.sample().item()\n",
    "    else:\n",
    "        init_idx[seed_len+kk] = torch.max(out[0, seed_len+kk],0)[1].item()\n",
    "        \n",
    "    if numpy.mod(ii, viz_int) == 0:\n",
    "        for_print = tokenizer.convert_ids_to_tokens(init_idx)\n",
    "        for_print = for_print[:seed_len+kk+1] + ['(*)'] + for_print[seed_len+kk+1:]\n",
    "        print(\"iter\", ii+1, \" \".join(for_print))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
