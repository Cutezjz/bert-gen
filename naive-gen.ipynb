{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained('bert-large-uncased')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK] york is the greatest city in the world . => . ||| rank of new 458\n",
      "new [MASK] is the greatest city in the world . => it ||| rank of york 741\n",
      "new york [MASK] the greatest city in the world . => - ||| rank of is 2\n",
      "new york is [MASK] greatest city in the world . => the ||| rank of the 1\n",
      "new york is the [MASK] city in the world . => largest ||| rank of greatest 9\n",
      "new york is the greatest [MASK] in the world . => city ||| rank of city 1\n",
      "new york is the greatest city [MASK] the world . => in ||| rank of in 1\n",
      "new york is the greatest city in [MASK] world . => the ||| rank of the 1\n",
      "new york is the greatest city in the [MASK] . => . ||| rank of world 2\n",
      "new york is the greatest city in the world [MASK] => . ||| rank of . 1\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "original_sent = 'new york is the greatest city in the world .'.lower().split()\n",
    "\n",
    "for ii in range(len(original_sent)):\n",
    "    new_sent = copy.copy(original_sent)\n",
    "    new_sent[ii] = '[MASK]'\n",
    "#     new_sent[ii] = tokenizer.convert_ids_to_tokens([numpy.random.randint(0, len(tokenizer.vocab))])[0]\n",
    "    out = model(torch.tensor([tokenizer.convert_tokens_to_ids(new_sent)]))\n",
    "    pred = tokenizer.convert_ids_to_tokens([out[0][ii].max(0)[1].item()])[0]\n",
    "    probs = out[0][ii].data.numpy()\n",
    "    rank = len(tokenizer.vocab) - numpy.argsort(numpy.argsort(probs))[tokenizer.convert_tokens_to_ids([original_sent[ii]])[0]]\n",
    "    print(\" \".join(new_sent), \"=>\", pred, '|||', 'rank of', original_sent[ii], rank)\n",
    "#     if pred == 'the':\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1996]),)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.where(probs == probs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27842, 1996, 30521)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.argsort()[1996], probs.argmax(), numpy.argsort(numpy.argsort(probs))[1996]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30522,)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.1242, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][ii].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the']"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([1996])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-11.9881, grad_fn=<MinBackward1>)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][ii].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.1242, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][ii][1996]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] display cooling system ; and \" external access distressed \" tenant . internal access composed of 16 storage locations ;\n",
      "[CLS] 15 . 26 . 18 - 16 . 18 , & - | f ##ost ##don , devon countryside .\n",
      "[CLS] coleman . larkin . ba ##bb ##itt . french . harrison . harris . davis . lee . brooks .\n",
      "[CLS] \" hey , \" prince charming said , the caressing instrument with its strong and lil ##ting singing voice .\n",
      "[CLS] 1995 : restaurant ( mt . vernon 66 , 12 miles from texas , in the red river valley .\n",
      "[CLS] excerpt from the \" musical theatre electronic sound engineering . \" headline of the 2014 nevada film festival competition .\n",
      "[CLS] 15 . rosen , israel . sovereign states : jordan , egypt , morocco , libya , united nations .\n",
      "[CLS] ( in latin ) ; ( in german ) ; ( in hungarian ) ; ( swedish ) ; ;\n",
      "[CLS] cia ##o . baby . baby . baby . baby . baby . baby . baby . baby no !\n",
      "[CLS] and the battle of the heights or the battle of the heights or the battle of the glen ##s ;\n"
     ]
    }
   ],
   "source": [
    "''' sequential generation: this one kinda works '''\n",
    "\n",
    "sample = True\n",
    "max_len = 20\n",
    "leed_out_len = 5 #max_len\n",
    "random_future = False\n",
    "top_k = 0 # set it to 0 if you don't want top_k\n",
    "n_samples = 10\n",
    "\n",
    "seed_text = '[CLS]'.split()\n",
    "seed_len = len(seed_text)\n",
    "\n",
    "for si in range(n_samples):\n",
    "    init_text = seed_text + ['[MASK]'] * max_len\n",
    "    init_idx = tokenizer.convert_tokens_to_ids(init_text)\n",
    "    if random_future:\n",
    "        for ii in range(max_len):\n",
    "            init_idx[seed_len+ii] = numpy.random.randint(0, len(tokenizer.vocab))\n",
    "\n",
    "    for ii in range(max_len):\n",
    "        out = model(torch.tensor([init_idx[:seed_len+ii+leed_out_len]+tokenizer.convert_tokens_to_ids(['[SEP]'])]))\n",
    "        if top_k > 0:\n",
    "            logits = out[0,seed_len+ii]\n",
    "            kth_vals, kth_idx = logits.topk(top_k)\n",
    "            dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
    "            init_idx[seed_len+ii] = kth_idx[dist.sample().item()].item()\n",
    "        else:\n",
    "            if sample:\n",
    "                dist = torch.distributions.categorical.Categorical(logits=out[0,seed_len+ii])\n",
    "                init_idx[seed_len+ii] = dist.sample().item()\n",
    "            else:\n",
    "                init_idx[seed_len+ii] = torch.max(out[0, seed_len+ii],0)[1].item()\n",
    "\n",
    "#     print(init_idx)\n",
    "    print(\" \".join(tokenizer.convert_ids_to_tokens(init_idx)))\n",
    "# print(\" \".join(tokenizer.convert_ids_to_tokens(init_idx)).replace(\" ##\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[102]"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(['[SEP]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1 [CLS] philippine \" ##hara ##id on mir by character sons five god with the , ; for a fatal ##in ; [SEP]\n",
      "iter 11 [CLS] 2 m be ##h on ##r by the to . god with . aid ; for present definite h . [SEP]\n",
      "iter 21 [CLS] 2 ##m be ##h on ##r by the to . god with . aid ; for present definite h . [SEP]\n",
      "iter 31 [CLS] 2 ##m be ##h on ##s by the to . god with . aid ; for present definite h . [SEP]\n",
      "iter 41 [CLS] 2 ##m be ##h on ze by the to . god with . aid ; for which definite h . [SEP]\n",
      "iter 51 [CLS] 2 ##m be ##h on - by the to . god with . aid ; p or an h . [SEP]\n",
      "iter 61 [CLS] 2 ##m be ##h on made by the to . god with . help the p or an h . [SEP]\n",
      "iter 71 [CLS] 2 ##b be ##h on made by the to . god with . help the p or an h . [SEP]\n",
      "iter 81 [CLS] 2 ##b be ##h on made by the to . god with . help the p or an h . [SEP]\n",
      "iter 91 [CLS] 2 ##b be ##h is made by the to . god with . help the p or an h . [SEP]\n"
     ]
    }
   ],
   "source": [
    "''' parallel generation: this one doesn't work '''\n",
    "\n",
    "sample = True\n",
    "max_iter = 100\n",
    "viz_int = 10\n",
    "max_len = 20\n",
    "top_k = 0\n",
    "\n",
    "seed_text = '[CLS]'.split()\n",
    "seed_len = len(seed_text)\n",
    "\n",
    "init_text = seed_text + ['[MASK]'] * max_len + ['[SEP]']\n",
    "init_idx = tokenizer.convert_tokens_to_ids(init_text)\n",
    "# for ii in range(max_len):\n",
    "#     init_idx[seed_len+ii] = numpy.random.randint(0, len(tokenizer.vocab))\n",
    "\n",
    "for ii in range(max_iter):\n",
    "    out = model(torch.tensor([init_idx]))\n",
    "    for kk in range(max_len):\n",
    "        if top_k > 0:\n",
    "            logits = out[0,seed_len+kk]\n",
    "            kth_vals, kth_idx = logits.topk(top_k)\n",
    "            dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
    "            init_idx[seed_len+kk] = kth_idx[dist.sample().item()].item()\n",
    "        else:\n",
    "            if sample:\n",
    "                dist = torch.distributions.categorical.Categorical(logits=out[0,seed_len+kk])\n",
    "                init_idx[seed_len+kk] = dist.sample().item()\n",
    "            else:\n",
    "                init_idx[seed_len+kk] = torch.max(out[0, seed_len+kk],0)[1].item()\n",
    "    if numpy.mod(ii, viz_int) == 0:\n",
    "        print(\"iter\", ii+1, \" \".join(tokenizer.convert_ids_to_tokens(init_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10 [CLS] though [MASK] [MASK] with regular deliveries , (*) ni [MASK] [MASK] [MASK] [MASK] [MASK] players . [SEP]\n",
      "iter 20 [CLS] despite playing [MASK] with regular deliveries , ho (*) ##ssa ##nick [MASK] [MASK] 16 wickets . [SEP]\n",
      "iter 30 [CLS] despite bowling strongly with swing bowling (*) , ho ##ssa ##in had [MASK] 16 wickets . [SEP]\n",
      "iter 40 [CLS] despite bowling better with stump ##ers , ho ##ssa ##in (*) had [MASK] 16 wickets . [SEP]\n",
      "iter 50 [CLS] despite out (*) slowly with stump ##ing , ho ##ssa ##in still [MASK] 16 wickets . [SEP]\n",
      "iter 60 [CLS] going out slowly after odi ##s , ho ##ssa ##in still [MASK] seventh place (*) . [SEP]\n",
      "iter 70 [CLS] starting off slowly in (*) odi ##s , ho ##ssa ##in finished in seventh overall . [SEP]\n",
      "iter 80 [CLS] starting (*) off well in sprint ##s , ho ##ssa ##in ended in seventh overall . [SEP]\n",
      "iter 90 [CLS] starting off strongly in sprint ##s , ho ##ssa ##ny (*) finished in eighth overall . [SEP]\n",
      "iter 100 [CLS] starting off again in 3rd places , ho ##s ##ny finished in (*) 6th position . [SEP]\n",
      "iter 110 [CLS] starting off again in 7th (*) place , ho ##s ##nard finished in 17th position . [SEP]\n",
      "iter 120 [CLS] starting once again in third place , ra ##d ##nard finished in 17th position . (*) [SEP]\n",
      "iter 130 [CLS] schmidt (*) once again in 8th position , ra ##d ##nard finished in 5th position . [SEP]\n",
      "iter 140 [CLS] edwards once again finished 12th position while ra ##d ##ja (*) finished in 5th position . [SEP]\n",
      "iter 150 [CLS] neal once again finished 12th position while ra ##d ##moor finished in 10th position . (*) [SEP]\n",
      "iter 160 [CLS] arrow (*) once again finished 12th overall . ra ##d ##moor finished in 21st position . [SEP]\n",
      "iter 170 [CLS] lock ##hart again finished 3rd overall while ra ##d ##nor finished in (*) 21st position . [SEP]\n",
      "iter 180 [CLS] flock ##hart again finished 3rd overall while (*) ra ##d ##nor finished in 21st position . [SEP]\n",
      "iter 190 [CLS] lock (*) ##hart again finished 3rd , while ra ##d ##nor finished in 4th position . [SEP]\n",
      "iter 200 [CLS] lock ##hart again finished fourth , and ra ##d ##nor (*) finished in 4th place . [SEP]\n",
      "iter 210 [CLS] lock ##hart again finished 2nd , and ra ##d ##nor finished (*) in 4th place . [SEP]\n",
      "iter 220 [CLS] barn ##hart again finished 2nd , while ra ##d ##nor finished (*) in 3rd place . [SEP]\n",
      "iter 230 [CLS] barn ##hart again finished 2nd (*) , while sy ##d ##nor finished in 3rd place . [SEP]\n",
      "iter 240 [CLS] lock ##hart ultimately finished 2nd , while (*) sy ##d ##nor finished in 3rd place . [SEP]\n",
      "iter 250 [CLS] lock ##hart ultimately finished 2nd , while ra ##d ##nor finished in 3rd (*) place . [SEP]\n",
      "iter 260 [CLS] lock ##hart ultimately finished 2nd , while ra ##d ##nor finished in 3rd place (*) . [SEP]\n",
      "iter 270 [CLS] lock ##hart ultimately finished 2nd , while (*) ra ##d ##nor finished in 3rd place . [SEP]\n",
      "iter 280 [CLS] lock ##hart also finished 2nd , while ra ##d ##nor finished in 3rd (*) place . [SEP]\n",
      "iter 290 [CLS] lock ##hart also finished 2nd , while ra ##d ##nor finished in 3rd (*) place . [SEP]\n",
      "iter 300 [CLS] lock ##hart also finished 2nd , while ra ##d ##nor finished (*) in 3rd place . [SEP]\n"
     ]
    }
   ],
   "source": [
    "''' parallel-sequential generation: this one definitely works '''\n",
    "\n",
    "# sample = True\n",
    "burnin = 200\n",
    "max_iter = 300\n",
    "viz_int = 10\n",
    "max_len = 15\n",
    "top_k = 0\n",
    "\n",
    "seed_text = '[CLS]'.split()\n",
    "seed_len = len(seed_text)\n",
    "\n",
    "init_text = seed_text + ['[MASK]'] * (max_len) + ['[SEP]']\n",
    "init_idx = tokenizer.convert_tokens_to_ids(init_text)\n",
    "# for ii in range(max_len):\n",
    "#     init_idx[seed_len+ii] = numpy.random.randint(0, len(tokenizer.vocab))\n",
    "\n",
    "for ii in range(max_iter):\n",
    "    kk = numpy.random.randint(0, max_len)\n",
    "    init_idx[seed_len+kk] = tokenizer.convert_tokens_to_ids(['[MASK]'])[0]\n",
    "    out = model(torch.tensor([init_idx]))\n",
    "    if top_k > 0:\n",
    "        logits = out[0,seed_len+kk]\n",
    "        kth_vals, kth_idx = logits.topk(top_k)\n",
    "        dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
    "        init_idx[seed_len+kk] = kth_idx[dist.sample().item()].item()\n",
    "    else:\n",
    "        if ii < burnin:\n",
    "            dist = torch.distributions.categorical.Categorical(logits=out[0,seed_len+kk])\n",
    "            init_idx[seed_len+kk] = dist.sample().item()\n",
    "        else:\n",
    "            init_idx[seed_len+kk] = torch.max(out[0, seed_len+kk],0)[1].item()\n",
    "        \n",
    "    if numpy.mod(ii+1, viz_int) == 0:\n",
    "        for_print = tokenizer.convert_ids_to_tokens(init_idx)\n",
    "        for_print = for_print[:seed_len+kk+1] + ['(*)'] + for_print[seed_len+kk+1:]\n",
    "        print(\"iter\", ii+1, \" \".join(for_print))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
