{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained('bert-large-uncased')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK] is the capital of china . => . ||| rank of beijing 19\n",
      "beijing [MASK] the capital of china . => - ||| rank of is 3\n",
      "beijing is [MASK] capital of china . => the ||| rank of the 1\n",
      "beijing is the [MASK] of china . => name ||| rank of capital 17\n",
      "beijing is the capital [MASK] china . => of ||| rank of of 1\n",
      "beijing is the capital of [MASK] . => canada ||| rank of china 8\n",
      "beijing is the capital of china [MASK] => . ||| rank of . 1\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "original_sent = 'beijing is the capital of china .'.lower().split()\n",
    "\n",
    "for ii in range(len(original_sent)):\n",
    "    new_sent = copy.copy(original_sent)\n",
    "    new_sent[ii] = '[MASK]'\n",
    "#     new_sent[ii] = tokenizer.convert_ids_to_tokens([numpy.random.randint(0, len(tokenizer.vocab))])[0]\n",
    "    out = model(torch.tensor([tokenizer.convert_tokens_to_ids(new_sent)]))\n",
    "    pred = tokenizer.convert_ids_to_tokens([out[0][ii].max(0)[1].item()])[0]\n",
    "    probs = out[0][ii].data.numpy()\n",
    "    rank = len(tokenizer.vocab) - numpy.argsort(numpy.argsort(probs))[tokenizer.convert_tokens_to_ids([original_sent[ii]])[0]]\n",
    "    print(\" \".join(new_sent), \"=>\", pred, '|||', 'rank of', original_sent[ii], rank)\n",
    "#     if pred == 'the':\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1996]),)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.where(probs == probs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27842, 1996, 30521)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.argsort()[1996], probs.argmax(), numpy.argsort(numpy.argsort(probs))[1996]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30522,)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.1242, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][ii].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the']"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([1996])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-11.9881, grad_fn=<MinBackward1>)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][ii].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.1242, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][ii][1996]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the meaning of life is in the world . \" \" no . \" . no . \" . \" . . . . '\n",
      "the meaning of life is gone . \" \" is . it , and the others of the group , have to . \" \"\n",
      "the meaning of life is . is . is . is . it . is . he . \" . . . i . \"\n",
      "the meaning of life is gone . \" , the the , \" the , \" \" \" . . \" , the \" the\n",
      "the meaning of life is in the right hands . \" \" yeah . i . i the , \" \" no . i did\n",
      "the meaning of life is . \" \" , . \" \" . . . . . . . . . . . . .\n",
      "the meaning of life is in heaven ! . the . the . i . the . the . the \" in \" her ,\n",
      "the meaning of life is gone . is dead . \" ; ; . . . \" , \" the other \" ; \" \"\n",
      "the meaning of life is gone . \" . . . . . . ; \" ; , \" , ) \" , \" ,\n",
      "the meaning of life is gone . \" \" no one , i \" \" . . . the . . . ' . .\n"
     ]
    }
   ],
   "source": [
    "''' sequential generation '''\n",
    "\n",
    "sample = True\n",
    "max_len = 20\n",
    "leed_out_len = 1 #max_len\n",
    "random_future = False\n",
    "top_k = 5 # set it to 0 if you don't want top_k\n",
    "n_samples = 10\n",
    "\n",
    "seed_text = 'the meaning of life is'.split()\n",
    "seed_len = len(seed_text)\n",
    "\n",
    "for si in range(n_samples):\n",
    "    init_text = seed_text + ['[MASK]'] * max_len\n",
    "    init_idx = tokenizer.convert_tokens_to_ids(init_text)\n",
    "    if random_future:\n",
    "        for ii in range(max_len):\n",
    "            init_idx[seed_len+ii] = numpy.random.randint(0, len(tokenizer.vocab))\n",
    "\n",
    "    for ii in range(max_len):\n",
    "        out = model(torch.tensor([init_idx[:seed_len+ii+leed_out_len]]))\n",
    "        if top_k > 0:\n",
    "            logits = out[0,seed_len+ii]\n",
    "            kth_vals, kth_idx = logits.topk(top_k)\n",
    "            dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
    "            init_idx[seed_len+ii] = kth_idx[dist.sample().item()].item()\n",
    "        else:\n",
    "            if sample:\n",
    "                dist = torch.distributions.categorical.Categorical(logits=out[0,seed_len+ii])\n",
    "                init_idx[seed_len+ii] = dist.sample().item()\n",
    "            else:\n",
    "                init_idx[seed_len+ii] = torch.max(out[0, seed_len+ii],0)[1].item()\n",
    "\n",
    "#     print(init_idx)\n",
    "    print(\" \".join(tokenizer.convert_ids_to_tokens(init_idx)))\n",
    "# print(\" \".join(tokenizer.convert_ids_to_tokens(init_idx)).replace(\" ##\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1 the meaning of life is just in on and as , some one part got a all and to around from from or that or\n",
      "iter 11 the meaning of life is just that that and like , like that and was a lot and all that from that was that or\n",
      "iter 21 the meaning of life is . . . and and , and i . and . . . . . . . . . .\n",
      "iter 31 the meaning of life is . . . . . . . . . . . . . . . . . . . .\n",
      "iter 41 the meaning of life is . . . . . . . . . . . . . . . . . . . .\n",
      "iter 51 the meaning of life is . . . . . . . . . . . . . . . . . . . .\n",
      "iter 61 the meaning of life is . . . . . . . . . . . . . . . . . . . .\n",
      "iter 71 the meaning of life is . . . . . . . . . . . . . . . . . . . .\n",
      "iter 81 the meaning of life is . . . . . . . . . . . . . . . . . . . .\n",
      "iter 91 the meaning of life is . . . . . . . . . . . . . . . . . . . .\n"
     ]
    }
   ],
   "source": [
    "''' parallel generation '''\n",
    "\n",
    "sample = True\n",
    "max_iter = 100\n",
    "viz_int = 10\n",
    "max_len = 20\n",
    "top_k = 5\n",
    "\n",
    "seed_text = 'the meaning of life is'.split()\n",
    "seed_len = len(seed_text)\n",
    "\n",
    "init_text = seed_text + ['[MASK]'] * max_len\n",
    "init_idx = tokenizer.convert_tokens_to_ids(init_text)\n",
    "for ii in range(max_len):\n",
    "    init_idx[seed_len+ii] = numpy.random.randint(0, len(tokenizer.vocab))\n",
    "\n",
    "for ii in range(max_iter):\n",
    "    out = model(torch.tensor([init_idx]))\n",
    "    for kk in range(max_len):\n",
    "        if top_k > 0:\n",
    "            logits = out[0,seed_len+kk]\n",
    "            kth_vals, kth_idx = logits.topk(top_k)\n",
    "            dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
    "            init_idx[seed_len+kk] = kth_idx[dist.sample().item()].item()\n",
    "        else:\n",
    "            if sample:\n",
    "                dist = torch.distributions.categorical.Categorical(logits=out[0,seed_len+kk])\n",
    "                init_idx[seed_len+kk] = dist.sample().item()\n",
    "            else:\n",
    "                init_idx[seed_len+kk] = torch.max(out[0, seed_len+kk],0)[1].item()\n",
    "    if numpy.mod(ii, viz_int) == 0:\n",
    "        print(\"iter\", ii+1, \" \".join(tokenizer.convert_ids_to_tokens(init_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1 the meaning of life is [MASK] [MASK] of (*) [MASK] [MASK]\n",
      "iter 11 the meaning of life is the meaning of (*) life [MASK]\n",
      "iter 21 the meaning of life is . meaning of life is (*)\n",
      "iter 31 the meaning of life is . end of (*) life .\n",
      "iter 41 the meaning of life is not (*) means of it in\n",
      "iter 51 the meaning of life is the (*) end of being .\n",
      "iter 61 the meaning of life is . meaning of (*) being .\n",
      "iter 71 the meaning of life is the meaning of death (*) .\n",
      "iter 81 the meaning of life is the meaning of life (*) is\n",
      "iter 91 the meaning of life is what (*) part of time is\n"
     ]
    }
   ],
   "source": [
    "''' parallel-sequential generation '''\n",
    "\n",
    "# sample = True\n",
    "burnin = 200\n",
    "max_iter = 200\n",
    "viz_int = 10\n",
    "max_len = 10\n",
    "top_k = 5\n",
    "\n",
    "seed_text = 'the meaning of life is'.split()\n",
    "seed_len = len(seed_text)\n",
    "\n",
    "init_text = seed_text + ['[MASK]'] * max_len\n",
    "init_idx = tokenizer.convert_tokens_to_ids(init_text)\n",
    "# for ii in range(max_len):\n",
    "#     init_idx[seed_len+ii] = numpy.random.randint(0, len(tokenizer.vocab))\n",
    "\n",
    "for ii in range(max_iter):\n",
    "    kk = numpy.random.randint(0, max_len)\n",
    "    init_idx[seed_len+kk] = tokenizer.convert_tokens_to_ids(['[MASK]'])[0]\n",
    "    out = model(torch.tensor([init_idx]))\n",
    "    if top_k > 0:\n",
    "        logits = out[0,seed_len+kk]\n",
    "        kth_vals, kth_idx = logits.topk(top_k)\n",
    "        dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
    "        init_idx[seed_len+kk] = kth_idx[dist.sample().item()].item()\n",
    "    else:\n",
    "        if ii < burnin:\n",
    "            dist = torch.distributions.categorical.Categorical(logits=out[0,seed_len+kk])\n",
    "            init_idx[seed_len+kk] = dist.sample().item()\n",
    "        else:\n",
    "            init_idx[seed_len+kk] = torch.max(out[0, seed_len+kk],0)[1].item()\n",
    "        \n",
    "    if numpy.mod(ii, viz_int) == 0:\n",
    "        for_print = tokenizer.convert_ids_to_tokens(init_idx)\n",
    "        for_print = for_print[:seed_len+kk+1] + ['(*)'] + for_print[seed_len+kk+1:]\n",
    "        print(\"iter\", ii+1, \" \".join(for_print))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
