{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model_version = 'bert-large-cased'\n",
    "model = BertForMaskedLM.from_pretrained(model_version)\n",
    "model.eval()\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=model_version.endswith(\"uncased\"))\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return [tokenizer.convert_tokens_to_ids(sent) for sent in batch]\n",
    "\n",
    "def untokenize_batch(batch):\n",
    "    return [tokenizer.convert_ids_to_tokens(sent) for sent in batch]\n",
    "\n",
    "def detokenize(sent):\n",
    "    \"\"\" Roughly detokenizes (mainly undoes wordpiece) \"\"\"\n",
    "    new_sent = []\n",
    "    for i, tok in enumerate(sent):\n",
    "        if tok.startswith(\"##\"):\n",
    "            new_sent[len(new_sent) - 1] = new_sent[len(new_sent) - 1] + tok[2:]\n",
    "        else:\n",
    "            new_sent.append(tok)\n",
    "    return new_sent\n",
    "\n",
    "CLS = '[CLS]'\n",
    "SEP = '[SEP]'\n",
    "MASK = '[MASK]'\n",
    "mask_id = tokenizer.convert_tokens_to_ids([MASK])[0]\n",
    "sep_id = tokenizer.convert_tokens_to_ids([MASK])[0]\n",
    "cls_id = tokenizer.convert_tokens_to_ids([MASK])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Generation modes as functions '''\n",
    "\n",
    "def generate_step(out, gen_idx, temperature=None, top_k=0, sample=False):\n",
    "    \"\"\" Generate a word from from out[gen_idx]\n",
    "    \n",
    "    args:\n",
    "        - out (torch.Tensor): tensor of logits of size batch_size x seq_len x vocab_size\n",
    "        - gen_idx (int): location for which to generate for\n",
    "        - top_k (int): if >0, only sample from the top k most probable words\n",
    "        - sample (Bool): if True, sample from full distribution. Overridden by top_k \n",
    "    \"\"\"\n",
    "    logits = out[:, gen_idx]\n",
    "    if temperature is not None:\n",
    "        logits = logits / temperature\n",
    "    if top_k > 0:\n",
    "        kth_vals, kth_idx = logits.topk(top_k, dim=-1)\n",
    "        dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
    "        idx = kth_idx.gather(dim=1, index=dist.sample().unsqueeze(-1)).squeeze(-1).tolist()\n",
    "    elif sample:\n",
    "        dist = torch.distributions.categorical.Categorical(logits=logits)\n",
    "        idx = dist.sample().squeeze(-1).tolist()\n",
    "    else:\n",
    "        idx = torch.argmax(logits, dim=-1).tolist()\n",
    "    return idx\n",
    "\n",
    "def get_init_text(seed_text, max_len, batch_size = 1, rand_init=False):\n",
    "    \"\"\" Get initial sentence by padding seed_text with either masks or random words to max_len \"\"\"\n",
    "    batch = [seed_text + [MASK] * max_len + [SEP] for _ in range(batch_size)]\n",
    "    #if rand_init:\n",
    "    #    for ii in range(max_len):\n",
    "    #        init_idx[seed_len+ii] = np.random.randint(0, len(tokenizer.vocab))\n",
    "    \n",
    "    return tokenize_batch(batch)\n",
    "\n",
    "def parallel_sequential_generation(seed_text, max_len=15, top_k=0, temperature=None, max_iter=300, burnin=200,\n",
    "                                   print_every=10, verbose=True):\n",
    "    \"\"\" Generate for one random position at a timestep\n",
    "    \n",
    "    args:\n",
    "        - burnin: during burn-in period, sample from full distribution; afterwards take argmax\n",
    "    \"\"\"\n",
    "    seed_len = len(seed_text)\n",
    "    batch = get_init_text(seed_text, max_len, batch_size)\n",
    "    \n",
    "    for ii in range(max_iter):\n",
    "        kk = np.random.randint(0, max_len)\n",
    "        for jj in range(batch_size):\n",
    "            batch[jj][seed_len+kk] = mask_id\n",
    "        out = model(torch.tensor(batch))\n",
    "        idxs = generate_step(out, gen_idx=seed_len+kk, top_k=top_k, sample=(ii < burnin))\n",
    "        for jj in range(batch_size):\n",
    "            batch[jj][seed_len+kk] = idxs[jj]\n",
    "            \n",
    "        if verbose and np.mod(ii+1, print_every) == 0:\n",
    "            for_print = tokenizer.convert_ids_to_tokens(batch[0])\n",
    "            for_print = for_print[:seed_len+kk+1] + ['(*)'] + for_print[seed_len+kk+1:]\n",
    "            print(\"iter\", ii+1, \" \".join(for_print))\n",
    "            \n",
    "    return untokenize_batch(batch)\n",
    "\n",
    "def parallel_generation(seed_text, max_len=15, top_k=0, temperature=None, max_iter=300, sample=True, \n",
    "                        print_every=10, verbose=True):\n",
    "    \"\"\" Generate for all positions at a time step \"\"\"\n",
    "    seed_len = len(seed_text)\n",
    "    batch = get_init_text(seed_text, max_len, batch_size)\n",
    "    \n",
    "    for ii in range(max_iter):\n",
    "        out = model(torch.tensor(batch))\n",
    "        for kk in range(max_len):\n",
    "            idxs = generate_step(out, gen_idx=seed_len+kk, top_k=top_k, sample=sample)\n",
    "            for jj in range(batch_size):\n",
    "                batch[jj][seed_len+kk] = idxs[jj]\n",
    "            \n",
    "        if verbose and np.mod(ii, print_every) == 0:\n",
    "            print(\"iter\", ii+1, \" \".join(tokenizer.convert_ids_to_tokens(batch[0])))\n",
    "    \n",
    "    return untokenize_batch(batch)\n",
    "            \n",
    "def sequential_generation(seed_text, batch_size=2, max_len=15, leed_out_len=15, \n",
    "                          top_k=0, temperature=None, sample=True):\n",
    "    \"\"\" Generate one word at a time, in L->R order \"\"\"\n",
    "    seed_len = len(seed_text)\n",
    "    batch = get_init_text(seed_text, max_len, batch_size)\n",
    "\n",
    "    for ii in range(max_len):\n",
    "        inp = [sent[:seed_len+ii+leed_out_len]+[sep_id] for sent in batch]\n",
    "        out = model(torch.tensor(inp))\n",
    "        idxs = generate_step(out, gen_idx=seed_len+ii, top_k=top_k, sample=sample)\n",
    "        for jj in range(batch_size):\n",
    "            batch[jj][seed_len+ii] = idxs[jj]\n",
    "        \n",
    "        return untokenize_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "def generate(n_samples, seed_text=\"[CLS]\", batch_size=10, max_len=25, \n",
    "             sample=True, top_k=100, temperature=1.0, burnin=200, max_iter=500,\n",
    "             print_every=1):\n",
    "    sentences = []\n",
    "    n_batches = math.ceil(n_samples / batch_size)\n",
    "    start_time = time.time()\n",
    "    for batch_n in range(n_batches):\n",
    "        batch = parallel_sequential_generation(seed_text, max_len=max_len, top_k=top_k,\n",
    "                                               temperature=temperature, burnin=burnin, max_iter=max_iter, \n",
    "                                               verbose=False)\n",
    "        \n",
    "        #batch = sequential_generation(seed_text, batch_size=20, max_len=max_len, top_k=top_k, temperature=temperature, leed_out_len=leed_out_len, sample=sample)\n",
    "        #batch = parallel_generation(seed_text, max_len=max_len, top_k=top_k, temperature=temperature, sample=sample, max_iter=max_iter)\n",
    "        \n",
    "        if (batch_n + 1) % print_every == 0:\n",
    "            print(\"Finished batch %d in %.3fs\" % (batch_n + 1, time.time() - start_time))\n",
    "            start_time = time.time()\n",
    "        \n",
    "        sentences += batch\n",
    "    return sentences\n",
    "\n",
    "def printer(sent, should_detokenize=True):\n",
    "    if should_detokenize:\n",
    "        sent = detokenize(sent)\n",
    "    print(\" \".join(sent[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished batch 0 in 1035.656s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-954c4e3f3ff9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m sents = generate(n_samples, seed_text=seed_text, batch_size=batch_size, max_len=max_len,\n\u001b[0;32m---> 16\u001b[0;31m                  sample=sample, top_k=top_k, temperature=temperature, burnin=burnin, max_iter=max_iter)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-880d69d337af>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(n_samples, seed_text, batch_size, max_len, sample, top_k, temperature, burnin, max_iter, print_every)\u001b[0m\n\u001b[1;32m     11\u001b[0m         batch = parallel_sequential_generation(seed_text, max_len=max_len, top_k=top_k,\n\u001b[1;32m     12\u001b[0m                                                \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mburnin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mburnin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                                                verbose=False)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m#batch = sequential_generation(seed_text, batch_size=20, max_len=max_len, top_k=top_k, temperature=temperature, leed_out_len=leed_out_len, sample=sample)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-92-ff97a1dfbd03>\u001b[0m in \u001b[0;36mparallel_sequential_generation\u001b[0;34m(seed_text, max_len, top_k, temperature, max_iter, burnin, print_every, verbose)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mjj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseed_len\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mkk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed_len\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mkk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mii\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mburnin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mjj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bert/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bert/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, masked_lm_labels)\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_lm_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask,\n\u001b[0;32m--> 758\u001b[0;31m                                        output_all_encoded_layers=False)\n\u001b[0m\u001b[1;32m    759\u001b[0m         \u001b[0mprediction_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bert/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bert/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    627\u001b[0m         encoded_layers = self.encoder(embedding_output,\n\u001b[1;32m    628\u001b[0m                                       \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m                                       output_all_encoded_layers=output_all_encoded_layers)\n\u001b[0m\u001b[1;32m    630\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bert/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bert/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0mall_encoder_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_module\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0mall_encoder_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bert/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bert/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bert/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bert/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bert/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bert/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bert/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_samples = 500\n",
    "batch_size = 25\n",
    "max_len = 20\n",
    "top_k = 100\n",
    "temperature= 1.0\n",
    "\n",
    "leed_out_len = 5 # max_len\n",
    "burnin = 200\n",
    "sample = True\n",
    "max_iter = 500\n",
    "\n",
    "# Choose the prefix context\n",
    "seed_text = \"[CLS]\".split()\n",
    "\n",
    "sents = generate(n_samples, seed_text=seed_text, batch_size=batch_size, max_len=max_len,\n",
    "                 sample=sample, top_k=top_k, temperature=temperature, burnin=burnin, max_iter=max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_file = 'generations-len20-burnin200-temp0.700.txt'\n",
    "sents = [detokenize(sent.strip().split()) for sent in open(sent_file).readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for men . men with beards who ploughed up and down . who worked hard\n",
      "towns include the large upper borre valley ( the val de borre ) and borre\n",
      "of the cops are savvy , \" he said . \" they found the murder weapon\n",
      "names , dates , holidays , birthdays . where to next ? \" the response was immediate\n",
      ". hoo . com . \" women in business : a survey and annual report \"\n",
      ", my dad was the one guy who pleaded with him to cut me off all the time\n",
      "video concert 1 - 16 / 2006 . tv video concert 2 - 16 / 2006 . dvd\n",
      "kit , bass , snare , tuba , trombone , cornet , trumpet , french horn\n",
      "flowers are distinctively yellow . subspecies heliopsis hookeri ( golden sunflower ) subsp\n",
      "mother and her new husband filled the house with pillows and blankets and drunken one - night stands\n",
      "which one ? ) esta agua nadie lo que no abrio el ano\n",
      "third - and his final - two books , bone to bone ii , were a national bestseller\n",
      "can see that they have hair and that they wear white clothes with red and black on them\n",
      "the soil , bacteria ( eubacteria ) continue to grow , allowing self - renewal\n",
      ": contractor : telecoms ; information technology . e : contractor : engineering ; technical support services\n",
      "- curated the exhibitions « fotos latinos » 2017 / 18 and 2018 / 19\n",
      "this case , he was also accused of money laundering and bank fraud , under liberian law\n",
      "george v medal , 1914 - 1918 , clasp no . 1 . symons 2001 , p\n",
      "known as picture book ( picture book for children ) . color : red / green / blue\n",
      "( hua guofeng , liu xiaobo ) [ europe / africa / asia ] eq\n",
      "friends , your family , as well the families of \" paolo \" and \" beppe \"\n",
      "functions are denoted by f ( + 1 ) , and are defined by 〈 fₙ 〉\n",
      ": eigenkultur der 2 . wissenschaft des erstes , vol\n",
      "panormus czernyi \" . pappus generosus . british library\n",
      "house , london . penguin classics . penguin books , london . penguin classics . woodcuts\n",
      "; ( former state representatives susana martinez and carlos martinez and senator jorge andrade ) independents\n",
      "is located in the main square that surrounds the city and is known as \" freedom square \"\n",
      ": the national teams of the indian subcontinent compete in an annual tournament organised by bbc world service\n",
      "/ added support for intel x86 ; / / released under gpl v2 . 0\n",
      "8 . \" funeral of duke albert of oldenburg , 30 may 1891 \" . catholic encyclopedia\n",
      "moment that she had been waiting for for a long time , but never with someone like him\n",
      ". a history of philosophy , volumes 1 and 2 : kant and the new materialist philosophy\n",
      ". 15 - orville sells his interest in the griersons to his own law firm\n",
      "of canada : fort laramie was built in 1876 after the battle of sand creek in wyoming\n",
      "/ 88 : name change implemented . it serves as a link between london bridge and london victoria\n",
      ", 2 . ( new york city , ny . ) ( see volumes 1 and 2 )\n",
      "average size of household was 20 males and 20 females : 10 white and 10 african american males\n",
      "had sex with a demon , but not with a half - demon trying to drink from him\n",
      "guy with the camcorder might be there too . ' ' really ? ' ' yeah\n",
      "bosco , australian film . neighbours , australian tv series . last resort , australian tv series\n",
      ". 25 - 1 . 26 lakanan people . they are called as lakanan\n",
      ", like king marduk , was also deeply religious and respected the traditions of the assyrians\n",
      "euston estate also has annexes ( detached houses ) known as \" estate annexes \"\n",
      "is very durable and , when used in place of wood , any imperfections are easily repaired\n",
      "included the discovery of respirator - free oxygen gas , the first known form of oxygen\n",
      "think about it , son . do you want to buy land with no trees or grass ?\n",
      "a lull they recruited drummer micky zito and former buddy holly bassist tony derosa\n",
      "died of arsenic poisoning and was buried in one of cuyahoga valley hot springs\n",
      "album covers were removed and the band opted for self - publishing the liner notes in digital format\n",
      "a ) 50 mhz carrier wave ; ( b ) 50 mhz carrier wave ; ( c )\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    printer(sents[i], should_detokenize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import bleu_score as bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How similar are the generated sentences to the original training data (Toronto Book Corpus and Wikipedia dumps). We follow Yu et al., () and compute the BLEU between the generations and the test sets of both corpora by treating the test set as the references for each generation. The tests sets are large; we subsample 5000 examples from each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_file, replacements={}, uncased=True):\n",
    "    data = [d.strip().split() for d in open(data_file, 'r').readlines()]\n",
    "    if uncased:\n",
    "        data = [[t.lower() for t in sent] for sent in data]\n",
    "        \n",
    "    for k, v in replacements.items():\n",
    "        data = [[t if t != k else v for t in sent] for sent in data]\n",
    " \n",
    "    return data\n",
    "\n",
    "def prepare_wiki(data_file, uncased=True):\n",
    "    replacements = {\"@@unknown@@\": \"[UNK]\"}\n",
    "    return prepare_data(data_file, replacements=replacements, uncased=uncased)\n",
    "\n",
    "def prepare_tbc(data_file):        \n",
    "    replacements = {\"``\": \"\\\"\", \"\\'\\'\": \"\\\"\"}\n",
    "    return prepare_data(data_file, replacements=replacements)\n",
    "\n",
    "def corpus_bleu(generated, references):\n",
    "    \"\"\" Compute similarity between two corpora as measured by\n",
    "    comparing each sentence of `generated` against all sentences in `references` \n",
    "    \n",
    "    args:\n",
    "        - generated (List[List[str]]): list of sentences (split into tokens)\n",
    "        - references (List[List[str]]): list of sentences (split into tokens)\n",
    "        \n",
    "    returns:\n",
    "        - bleu (float)\n",
    "    \"\"\"    \n",
    "    return bleu.corpus_bleu([references for _ in range(len(generated))], generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki103_file = 'data/wiki103.5k.txt'\n",
    "tbc_file = 'data/tbc.5k.txt'\n",
    "\n",
    "wiki_data = prepare_wiki(wiki103_file)\n",
    "tbc_data = prepare_tbc(tbc_file)\n",
    "#sents = [detokenize(sent) for sent in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT-TBC BLEU: 8.30\n",
      "BERT-Wiki103 BLEU: 11.00\n",
      "BERT-{TBC + Wiki103} BLEU: 11.29\n"
     ]
    }
   ],
   "source": [
    "print(\"BERT-TBC BLEU: %.2f\" % (100 * corpus_bleu(sents, tbc_data)))\n",
    "print(\"BERT-Wiki103 BLEU: %.2f\" % (100 * corpus_bleu(sents, wiki_data)))\n",
    "print(\"BERT-{TBC + Wiki103} BLEU: %.2f\" % (100 * corpus_bleu(sents, tbc_data[:2500] + wiki_data[:2500])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity of a Trained LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A proxy for measuring the fluency of the generated sentences is computing the perplexity of a well-trained language model. We'll use the Gated Convolution Language Model from Dauphin et al. (2016). The model we're using was trained on WikiText-103 (Merity et al., 2016), so it's biased in favor of BERT generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing to existing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OpenAI Generative Pretraining Transformer is another pretrained model successfully used for transfer learning. Since the model is a unidirectional language model, we can straightforwardly generate from the model. See [this repo](https://github.com/huggingface/pytorch-openai-transformer-lm) by Thomas Wolf at Huggingface for instructions for setting up the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, os.path.join(\".\", \"pytorch-openai-transformer-lm\"))\n",
    "\n",
    "from model_pytorch import LMModel, load_openai_pretrained_model, DEFAULT_CONFIG\n",
    "from text_utils import TextEncoder\n",
    "\n",
    "def load_openai_gpt(n_special=1, n_ctx=512):\n",
    "    text_encoder = TextEncoder(\"pytorch-openai-transformer-lm/model/encoder_bpe_40000.json\", \n",
    "                               \"pytorch-openai-transformer-lm/model/vocab_40000.bpe\")\n",
    "    encoder = text_encoder.encoder\n",
    "    n_vocab = len(text_encoder.encoder)\n",
    "    vocab = n_vocab + n_special + n_ctx\n",
    "\n",
    "    args = DEFAULT_CONFIG\n",
    "    lm_model = LMModel(args, vocab, n_ctx, return_probs=True)\n",
    "    load_openai_pretrained_model(lm_model.transformer, n_ctx=n_ctx, n_special=n_special,\n",
    "                                 path=\"pytorch-openai-transformer-lm/model/\",\n",
    "                                 path_names=\"pytorch-openai-transformer-lm/\")\n",
    "    #lm_model.to(device)\n",
    "    lm_model.eval()\n",
    "    return lm_model, text_encoder\n",
    "\n",
    "def make_batch(X, n_vocab, n_special):\n",
    "    X = np.array(X)\n",
    "    assert X.ndim in [1, 2]\n",
    "    if X.ndim == 1:\n",
    "        X = np.expand_dims(X, axis=0)\n",
    "    pos_enc = np.arange(n_vocab + n_special, n_vocab + n_special + X.shape[-1])\n",
    "    pos_enc = np.expand_dims(pos_enc, axis=0)\n",
    "    batch = np.stack([X, pos_enc], axis=-1)\n",
    "    batch = torch.tensor(batch, dtype=torch.long)#.to(device)\n",
    "    return batch\n",
    "\n",
    "def append_batch(X, next_idx):\n",
    "    next_pos = X[:, -1:, 1] + 1\n",
    "    next_x = torch.cat((next_idx, next_pos), -1).unsqueeze(1)\n",
    "    return torch.cat((X, next_x), 1)\n",
    "\n",
    "def generate_sentence_openai(model, text_encoder, seed_text, gen_len=20, topk=100, \n",
    "                             n_vocab=40478, n_special=0, verbose=False):\n",
    "    X = [[n_vocab - 1]]\n",
    "    if seed_text:\n",
    "        seed_ids = text_encoder.encode([seed_text,])\n",
    "        X = [X[0] + seed_ids[0]]\n",
    "        \n",
    "    n_vocab = len(text_encoder.encoder)\n",
    "    XMB = make_batch(X, n_vocab, n_special)\n",
    "    sent = [seed_text]\n",
    "\n",
    "    for _ in range(gen_len):\n",
    "        lm_probs = model(XMB)\n",
    "        if topk == 0:\n",
    "            next_idx = torch.multinomial(lm_probs[:, -1, :], 1)\n",
    "        else:\n",
    "            values, indices = lm_probs[:, -1, :].topk(topk)\n",
    "            next_idx = indices.gather(-1, torch.multinomial(values, 1))\n",
    "        next_token = text_encoder.decoder[next_idx.item()].replace('</w>', '')\n",
    "        sent.append(next_token)\n",
    "        if verbose:\n",
    "            print(next_token, end=' ')\n",
    "        XMB = append_batch(XMB, next_idx)\n",
    "        \n",
    "    return [tok for tok in sent if tok != '\\n']\n",
    "\n",
    "def generate_openai(model, text_encoder, n_samples, seed_text, gen_len=20, topk=100, \n",
    "                    n_vocab=40478, n_special=0, print_every=10):\n",
    "    sents = []\n",
    "    start_time = time.time()\n",
    "    for sample_n in range(n_samples):\n",
    "        sent = generate_sentence_openai(model, text_encoder, seed_text,\n",
    "                                        gen_len=gen_len, topk=topk, \n",
    "                                        n_vocab=n_vocab, n_special=n_special,\n",
    "                                        verbose=False)\n",
    "        sents.append(sent)\n",
    "        if (sample_n + 1) % print_every == 0:\n",
    "            print(\"Generated %d (#%d) of %d sentences in %.3fs\" % (print_every, sample_n + 1, n_samples, time.time() - start_time))\n",
    "            start_time = time.time()\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights...\n"
     ]
    }
   ],
   "source": [
    "gpt_model, gpt_text_encoder = load_openai_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 10 (#10) of 100 sentences in 12.369s\n",
      "Generated 10 (#20) of 100 sentences in 11.622s\n",
      "Generated 10 (#30) of 100 sentences in 11.162s\n",
      "Generated 10 (#40) of 100 sentences in 10.635s\n",
      "Generated 10 (#50) of 100 sentences in 11.085s\n",
      "Generated 10 (#60) of 100 sentences in 10.553s\n",
      "Generated 10 (#70) of 100 sentences in 10.563s\n",
      "Generated 10 (#80) of 100 sentences in 10.687s\n",
      "Generated 10 (#90) of 100 sentences in 11.073s\n",
      "Generated 10 (#100) of 100 sentences in 11.521s\n"
     ]
    }
   ],
   "source": [
    "openai_sents = generate_openai(gpt_model, gpt_text_encoder, 100, \"\", print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \" because ? \" \" just tell me . \" \" just tell you how to get in\n",
      " \" we will ... be ... prepared here . \" the queen 's voice rang out , so loud and\n",
      " he tried to pull away but she would n't let him go . she wanted him now . she 'd\n",
      " \" have i been sent here for some reason , brother ? \" he walked to a spot and sat\n",
      " \" yes , ' tis right , \" she agreed . \" though you - \" she looked from one\n",
      " \" hey ! \" the girl shrieked . \" do n't you even think about it . you know what\n",
      " \" what 's the matter ? \" she asked , genuinely concerned . \" been a bad day ? \"\n",
      " she shook her head . \" i think i 'm going to get some sleep . \" \" you\n",
      " \" are you sure ? \" he held out his hand . she shook her head , suddenly exhausted\n",
      " \" you are the most interesting person i 've ever met . i do n't know how you manage to\n",
      " \" we know the names of everyone involved , \" said marante . \" we know their families , histories\n",
      " \" i might have figured that one out myself , \" ty muttered . he pushed himself upright and rested\n",
      " \" fine , \" he says , standing up and pulling me to my feet . he holds my hand\n",
      " \" you were telling me about your cousin now ? \" \" it 's time you know , \"\n",
      " \" i 'm sorry , \" he gasped , squeezing her tightly . \" why ? we just met\n",
      " i pulled the cover off my glass and lifted my mug to my lips once more . \" another\n",
      " \" hey , \" i say as i stand , letting her walk away . \" do n't worry about\n",
      " \" oh , \" said sarah , getting a funny feeling about this all . \" then maybe it does\n",
      " \" well , we ought to go home and get changed , \" i said . i was grateful not\n",
      " \" let 's get them out of here . \" * the doors led to a dimly lit\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(\" \".join(openai_sents[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-TBC BLEU: 30.93\n",
      "GPT-Wiki103 BLEU: 12.55\n",
      "GPT-{TBC + Wiki103} BLEU: 26.64\n"
     ]
    }
   ],
   "source": [
    "print(\"GPT-TBC BLEU: %.2f\" % (100 * corpus_bleu(openai_sents, tbc_data)))\n",
    "print(\"GPT-Wiki103 BLEU: %.2f\" % (100 * corpus_bleu(openai_sents, wiki_data)))\n",
    "print(\"GPT-{TBC + Wiki103} BLEU: %.2f\" % (100 * corpus_bleu(openai_sents, tbc_data[:2500] + wiki_data[:2500])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diversity Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-BLEU: treat each sentence as a hypothesis and treat rest of corpus as reference. Lower is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_bleu(sents):\n",
    "    return bleu.corpus_bleu([[s for (j, s) in enumerate(sents) if j != i] for i in range(len(sents))], sents)\n",
    "\n",
    "def count_ngrams(max_n=4):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-BLEU: 9.59\n",
      "Self-BLEU: 19.96\n"
     ]
    }
   ],
   "source": [
    "print(\"BERT self-BLEU: %.2f\" % (100 * self_bleu(sents)))\n",
    "print(\"OpenAI self-BLEU: %.2f\" % (100 * self_bleu(openai_sents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Scratch ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality measure via outside language models\n",
    "\n",
    "# KN5 (KenLM)\n",
    "# pip install https://github.com/kpu/kenlm/archive/master.zip\n",
    "\n",
    "# Gated Convolutional LM (Fairseq)\n",
    "# https://github.com/pytorch/fairseq/blob/master/examples/language_model/README.md\n",
    "\n",
    "# OpenAI Generative Pretraining LM\n",
    "# https://github.com/huggingface/pytorch-openai-transformer-lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A man of wordly wealth , Sansom was primarily a business man but was also a politician .'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STR = \"A man of wordly wealth, Sansom was primarily a business man but was also a politician.\"\n",
    "\" \".join(detokenize(tokenizer.tokenize(STR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sentence 25 in 60.228s\n",
      "Generated sentence 50 in 57.656s\n",
      "Generated sentence 75 in 57.172s\n",
      "Generated sentence 100 in 57.339s\n",
      "Generated sentence 125 in 58.277s\n",
      "Generated sentence 150 in 58.121s\n",
      "Generated sentence 175 in 58.085s\n",
      "Generated sentence 200 in 57.711s\n",
      "Generated sentence 225 in 57.998s\n",
      "Generated sentence 250 in 57.194s\n",
      "Generated sentence 275 in 57.329s\n",
      "Generated sentence 300 in 58.291s\n",
      "Generated sentence 325 in 57.264s\n",
      "Generated sentence 350 in 57.242s\n",
      "Generated sentence 375 in 57.198s\n",
      "Generated sentence 400 in 57.487s\n",
      "Generated sentence 425 in 62.306s\n",
      "Generated sentence 450 in 57.114s\n",
      "Generated sentence 475 in 57.273s\n",
      "Generated sentence 500 in 57.203s\n",
      "Generated 500 sentences in 482.908m (~57.949s/sentence)\n",
      "Generated sentence 25 in 58.081s\n",
      "Generated sentence 50 in 58.023s\n",
      "Generated sentence 75 in 57.322s\n",
      "Generated sentence 100 in 58.552s\n",
      "Generated sentence 125 in 58.004s\n",
      "Generated sentence 150 in 57.925s\n",
      "Generated sentence 175 in 57.623s\n",
      "Generated sentence 200 in 58.549s\n",
      "Generated sentence 225 in 57.307s\n",
      "Generated sentence 250 in 57.277s\n",
      "Generated sentence 275 in 58.140s\n",
      "Generated sentence 300 in 57.301s\n",
      "Generated sentence 325 in 57.392s\n",
      "Generated sentence 350 in 58.511s\n",
      "Generated sentence 375 in 57.165s\n",
      "Generated sentence 400 in 57.304s\n",
      "Generated sentence 425 in 57.957s\n",
      "Generated sentence 450 in 57.842s\n",
      "Generated sentence 475 in 57.512s\n",
      "Generated sentence 500 in 65.998s\n",
      "Generated 500 sentences in 484.398m (~58.128s/sentence)\n",
      "Generated sentence 25 in 63.436s\n",
      "Generated sentence 50 in 72.187s\n",
      "Generated sentence 75 in 75.261s\n",
      "Generated sentence 100 in 70.779s\n",
      "Generated sentence 125 in 64.371s\n",
      "Generated sentence 150 in 74.569s\n",
      "Generated sentence 175 in 57.940s\n",
      "Generated sentence 200 in 71.494s\n",
      "Generated sentence 225 in 59.047s\n",
      "Generated sentence 250 in 64.907s\n",
      "Generated sentence 275 in 72.105s\n",
      "Generated sentence 300 in 68.935s\n",
      "Generated sentence 325 in 71.085s\n",
      "Generated sentence 350 in 78.970s\n",
      "Generated sentence 375 in 66.321s\n",
      "Generated sentence 400 in 66.812s\n",
      "Generated sentence 425 in 61.138s\n",
      "Generated sentence 450 in 61.238s\n",
      "Generated sentence 475 in 61.369s\n",
      "Generated sentence 500 in 60.276s\n",
      "Generated 500 sentences in 578.444m (~69.413s/sentence)\n",
      "Generated sentence 25 in 62.137s\n",
      "Generated sentence 50 in 65.650s\n",
      "Generated sentence 75 in 67.994s\n",
      "Generated sentence 100 in 61.786s\n",
      "Generated sentence 125 in 60.044s\n",
      "Generated sentence 150 in 60.579s\n",
      "Generated sentence 175 in 61.495s\n",
      "Generated sentence 200 in 63.405s\n",
      "Generated sentence 225 in 62.274s\n",
      "Generated sentence 250 in 62.260s\n",
      "Generated sentence 275 in 62.841s\n",
      "Generated sentence 300 in 59.860s\n",
      "Generated sentence 325 in 65.125s\n",
      "Generated sentence 350 in 60.194s\n",
      "Generated sentence 375 in 61.357s\n",
      "Generated sentence 400 in 66.472s\n",
      "Generated sentence 425 in 61.892s\n",
      "Generated sentence 450 in 62.492s\n",
      "Generated sentence 475 in 63.816s\n",
      "Generated sentence 500 in 59.070s\n",
      "Generated 500 sentences in 522.725m (~62.727s/sentence)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Get some generations \"\"\"\n",
    "import time\n",
    "\n",
    "n_sample = 500\n",
    "max_len = 20\n",
    "top_k = 0\n",
    "temperature = 1.\n",
    "burnin = 200\n",
    "max_iter = 400\n",
    "print_every = 25\n",
    "\n",
    "for top_k in [100]:\n",
    "    for temp in [.1, .5, .7, 2.]:\n",
    "        if top_k:\n",
    "            out_file = \"generations-len%d-topk%d-temp%.3f.txt\" % (max_len, top_k, temp)\n",
    "        else:\n",
    "            out_file = \"generations-len%d-burnin%d-temp%.3f.txt\" % (max_len, burnin, temp)\n",
    "\n",
    "        times = []\n",
    "        with open(out_file, \"w\") as out_fh:\n",
    "            start_time = time.time()\n",
    "            for step_n in range(n_sample):\n",
    "                seed_text = \"[CLS]\".split()\n",
    "                sent = parallel_sequential_generation(seed_text, max_len=max_len, \n",
    "                                                      top_k=top_k, temperature=temp, \n",
    "                                                      burnin=burnin, max_iter=max_iter,\n",
    "                                                      verbose=False)\n",
    "                out_fh.write(\"%s\\n\" % \" \".join(sent[1:-1]))\n",
    "                times.append(time.time() - start_time)\n",
    "                start_time = time.time()\n",
    "                if (step_n + 1) % print_every == 0:\n",
    "                    print(\"Generated sentence %d in %.3fs\" % (step_n + 1, times[-1]))\n",
    "\n",
    "        print(\"Generated %d sentences in %.3fm (~%.3fs/sentence)\" % (n_sample, sum(times) / 60, sum(times) / len(times)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [MASK] york is the greatest city in the world . [SEP] => new ||| rank of new 1\n",
      "[CLS] new [MASK] is the greatest city in the world . [SEP] => york ||| rank of york 1\n",
      "[CLS] new york [MASK] the greatest city in the world . [SEP] => is ||| rank of is 1\n",
      "[CLS] new york is [MASK] greatest city in the world . [SEP] => the ||| rank of the 1\n",
      "[CLS] new york is the [MASK] city in the world . [SEP] => largest ||| rank of greatest 15\n",
      "[CLS] new york is the greatest [MASK] in the world . [SEP] => city ||| rank of city 1\n",
      "[CLS] new york is the greatest city [MASK] the world . [SEP] => in ||| rank of in 1\n",
      "[CLS] new york is the greatest city in [MASK] world . [SEP] => the ||| rank of the 1\n",
      "[CLS] new york is the greatest city in the [MASK] . [SEP] => world ||| rank of world 1\n",
      "[CLS] new york is the greatest city in the world [MASK] [SEP] => . ||| rank of . 1\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "original_sent = [CLS] + 'new york is the greatest city in the world . '.lower().split() + [SEP]\n",
    "\n",
    "for ii_ in range(len(original_sent)-2):\n",
    "    ii = ii_ + 1\n",
    "    new_sent = copy.copy(original_sent)\n",
    "    new_sent[ii] = '[MASK]'\n",
    "#     new_sent[ii] = tokenizer.convert_ids_to_tokens([numpy.random.randint(0, len(tokenizer.vocab))])[0]\n",
    "    out = model(torch.tensor([tokenizer.convert_tokens_to_ids(new_sent)]))\n",
    "    pred = tokenizer.convert_ids_to_tokens([out[0][ii].max(0)[1].item()])[0]\n",
    "    probs = out[0][ii].data.numpy()\n",
    "    rank = len(tokenizer.vocab) - numpy.argsort(numpy.argsort(probs))[tokenizer.convert_tokens_to_ids([original_sent[ii]])[0]]\n",
    "    print(\" \".join(new_sent), \"=>\", pred, '|||', 'rank of', original_sent[ii], rank)\n",
    "#     if pred == 'the':\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" . . . . . , . . , . . . . . , . . . . [MASK]\n",
      "\" and Felix - ( - ) = + . - = . he = . - = . . [MASK]\n",
      "the and formula ##s ; , and - algebra ; ; . . . . . ; . . . [MASK]\n",
      "/ was . by . . . . . . and , from , gave . . . . . [MASK]\n",
      "* by army use of and as the of were applied as ( ( ) , and = ) . [MASK]\n",
      ". and ##i . . , . . . and ... , . . , to the part , . [MASK]\n",
      ". you ; ; ; ; ; ' Mr . Scott to the and ##ra of the . \" . [MASK]\n",
      ". . ##1 . : and . . . : . . : . . . . . . . [MASK]\n",
      "king - as of 2014 . | . / _ . / < < - | . / > | [MASK]\n",
      ". ##2 = . . = = = = - . = = = = ( = = ) | [MASK]\n"
     ]
    }
   ],
   "source": [
    "''' sequential generation: this one kinda works '''\n",
    "\n",
    "\n",
    "sep_id = tokenizer.convert_tokens_to_ids([SEP])\n",
    "sample = True\n",
    "max_len = 20\n",
    "leed_out_len = 5 #max_len\n",
    "random_future = False\n",
    "top_k = 100 # set it to 0 if you don't want top_k\n",
    "n_samples = 1\n",
    "\n",
    "seed_text = [[CLS] for _ in range(batch_size)]\n",
    "seed_len = len(seed_text[0])\n",
    "\n",
    "for si in range(n_samples):\n",
    "    #init_text = seed_text + ['[MASK]'] * max_len\n",
    "    init_text = [seed + ['[MASK]'] * max_len for seed in seed_text]\n",
    "    init_idx = tokenize_batch(init_text) #tokenizer.convert_tokens_to_ids(init_text)\n",
    "    #if random_future:\n",
    "    #    for ii in range(max_len):\n",
    "    #        init_idx[seed_len+ii] = numpy.random.randint(0, len(tokenizer.vocab))\n",
    "\n",
    "    for ii in range(max_len):\n",
    "        out = model(torch.tensor([i[:seed_len+ii+leed_out_len]+sep_id for i in init_idx]))\n",
    "        if top_k > 0:\n",
    "            logits = out[:,seed_len+ii]\n",
    "            kth_vals, kth_idx = logits.topk(top_k, dim=1)\n",
    "            dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
    "            new_idxs = kth_idx.gather(dim=1, index=dist.sample().unsqueeze(-1)).squeeze(-1).tolist()\n",
    "            for jj in range(len(init_idx)):\n",
    "                init_idx[jj][ii] = new_idxs[jj]\n",
    "        else:\n",
    "            if sample:\n",
    "                dist = torch.distributions.categorical.Categorical(logits=out[0,seed_len+ii])\n",
    "                init_idx[seed_len+ii] = dist.sample().item()\n",
    "            else:\n",
    "                init_idx[seed_len+ii] = torch.max(out[0, seed_len+ii],0)[1].item()\n",
    "\n",
    "#     print(init_idx)\n",
    "    for sent in init_idx:\n",
    "        print(\" \".join(tokenizer.convert_ids_to_tokens(sent)))\n",
    "# print(\" \".join(tokenizer.convert_ids_to_tokens(init_idx)).replace(\" ##\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[119], [119], [1103], [170], [168], [1110], [119], [176], [119], [119]]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1 [CLS] philippine \" ##hara ##id on mir by character sons five god with the , ; for a fatal ##in ; [SEP]\n",
      "iter 11 [CLS] 2 m be ##h on ##r by the to . god with . aid ; for present definite h . [SEP]\n",
      "iter 21 [CLS] 2 ##m be ##h on ##r by the to . god with . aid ; for present definite h . [SEP]\n",
      "iter 31 [CLS] 2 ##m be ##h on ##s by the to . god with . aid ; for present definite h . [SEP]\n",
      "iter 41 [CLS] 2 ##m be ##h on ze by the to . god with . aid ; for which definite h . [SEP]\n",
      "iter 51 [CLS] 2 ##m be ##h on - by the to . god with . aid ; p or an h . [SEP]\n",
      "iter 61 [CLS] 2 ##m be ##h on made by the to . god with . help the p or an h . [SEP]\n",
      "iter 71 [CLS] 2 ##b be ##h on made by the to . god with . help the p or an h . [SEP]\n",
      "iter 81 [CLS] 2 ##b be ##h on made by the to . god with . help the p or an h . [SEP]\n",
      "iter 91 [CLS] 2 ##b be ##h is made by the to . god with . help the p or an h . [SEP]\n"
     ]
    }
   ],
   "source": [
    "''' parallel generation: this one doesn't work '''\n",
    "\n",
    "sample = True\n",
    "max_iter = 100\n",
    "viz_int = 10\n",
    "max_len = 20\n",
    "top_k = 0\n",
    "\n",
    "seed_text = '[CLS]'.split()\n",
    "seed_len = len(seed_text)\n",
    "\n",
    "init_text = seed_text + ['[MASK]'] * max_len + ['[SEP]']\n",
    "init_idx = tokenizer.convert_tokens_to_ids(init_text)\n",
    "# for ii in range(max_len):\n",
    "#     init_idx[seed_len+ii] = numpy.random.randint(0, len(tokenizer.vocab))\n",
    "\n",
    "for ii in range(max_iter):\n",
    "    out = model(torch.tensor([init_idx]))\n",
    "    for kk in range(max_len):\n",
    "        if top_k > 0:\n",
    "            logits = out[0,seed_len+kk]\n",
    "            kth_vals, kth_idx = logits.topk(top_k)\n",
    "            dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
    "            init_idx[seed_len+kk] = kth_idx[dist.sample().item()].item()\n",
    "        else:\n",
    "            if sample:\n",
    "                dist = torch.distributions.categorical.Categorical(logits=out[0,seed_len+kk])\n",
    "                init_idx[seed_len+kk] = dist.sample().item()\n",
    "            else:\n",
    "                init_idx[seed_len+kk] = torch.max(out[0, seed_len+kk],0)[1].item()\n",
    "    if numpy.mod(ii, viz_int) == 0:\n",
    "        print(\"iter\", ii+1, \" \".join(tokenizer.convert_ids_to_tokens(init_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10 [CLS] un (*) ##i [MASK] [MASK] [MASK] ; [MASK] . [MASK] ##i [MASK] : ; [MASK] [MASK] [SEP]\n",
      "iter 20 [CLS] xx ##ix [MASK] ; [MASK] . [MASK] . (*) [MASK] . [MASK] 2 ; b [MASK] [SEP]\n",
      "iter 30 [CLS] vi (*) . [MASK] ; [MASK] . [MASK] . 17 . § 2 : ii . [SEP]\n",
      "iter 40 [CLS] iii . 11 ; norway . iii . 17 . § 87 (*) . 1 . [SEP]\n",
      "iter 50 [CLS] iii . (*) sweden . norway § 11 . 17 & § 87 . 12 . [SEP]\n",
      "iter 60 [CLS] iii . denmark (*) & norway § 11 . 17 . § 87 . 20 . [SEP]\n",
      "iter 70 [CLS] iii . denmark & norway § 87 . 17 ; § 87 . (*) 20 ; [SEP]\n",
      "iter 80 [CLS] 4 . denmark & norway § 87 (*) . 17 ; § 87 . 20 ; [SEP]\n",
      "iter 90 [CLS] 4 . denmark & sweden § (*) 85 . 11 ; § 87 . 20 ; [SEP]\n",
      "iter 100 [CLS] 4 - denmark & norway (*) § 86 . 6 ; § 86 . 2 ; [SEP]\n",
      "iter 110 [CLS] cf . denmark - (*) norway § 86 . 5 ; § 86 . 7 ; [SEP]\n",
      "iter 120 [CLS] cf . denmark - schleswig (*) § 1886 . 5 , § 86 . 1 ; [SEP]\n",
      "iter 130 [CLS] cf . denmark v schleswig § 86 . (*) 5 , § 86 . 1 ; [SEP]\n",
      "iter 140 [CLS] cf . (*) denmark ser . § 86 . 2 , § 86 . 6 ; [SEP]\n",
      "iter 150 [CLS] cf . denmark ser . § 86 . 0 , § 86 . 2 ; (*) [SEP]\n",
      "iter 160 [CLS] em . denmark 56 (*) . § 86 . 0 ; § 86 . 45 ; [SEP]\n",
      "iter 170 [CLS] 1 (*) . denmark 56 . § 86 . 42 ; § 86 . 45 ; [SEP]\n",
      "iter 180 [CLS] 5 . ch 56 . § 86 . 42 , § 86 (*) . 45 . [SEP]\n",
      "iter 190 [CLS] 5 . ch ##s . § 86 . 35 , § (*) 86 . 45 . [SEP]\n",
      "iter 200 [CLS] 5 . pre ##s . § 86 . 35 and § 86 . 45 . (*) [SEP]\n",
      "iter 210 [CLS] 5 . con ##s . § 86 . 44 . (*) § 86 . 45 . [SEP]\n",
      "iter 220 [CLS] 5 . con ##st . § (*) 86 . 44 . § 86 . 45 . [SEP]\n",
      "iter 230 [CLS] 5 . di ##st . § (*) 86 . 44 , § 86 . 45 . [SEP]\n",
      "iter 240 [CLS] 5 . di ##st . § 86 . 44 , § (*) 86 . 45 . [SEP]\n",
      "iter 250 [CLS] stat . di ##st . § 86 . 44 (*) . § 86 . 45 . [SEP]\n",
      "iter 260 [CLS] stat . di ##st . § 86 . 44 . § 86 . 45 . (*) [SEP]\n",
      "iter 270 [CLS] stat . di ##st . § (*) 86 . 44 . § 86 . 45 . [SEP]\n",
      "iter 280 [CLS] stat . di ##st . § 86 (*) . 44 . § 86 . 45 . [SEP]\n",
      "iter 290 [CLS] stat . di ##st . § 86 . (*) 44 . § 86 . 45 . [SEP]\n",
      "iter 300 [CLS] stat (*) . di ##st . § 86 . 44 . § 86 . 45 . [SEP]\n"
     ]
    }
   ],
   "source": [
    "''' parallel-sequential generation: this one definitely works '''\n",
    "\n",
    "# sample = True\n",
    "burnin = 200\n",
    "max_iter = 300\n",
    "viz_int = 10\n",
    "max_len = 15\n",
    "top_k = 0\n",
    "\n",
    "seed_text = '[CLS]'.split()\n",
    "seed_len = len(seed_text)\n",
    "\n",
    "init_text = seed_text + ['[MASK]'] * (max_len) + ['[SEP]']\n",
    "init_idx = tokenizer.convert_tokens_to_ids(init_text)\n",
    "#for ii in range(max_len):\n",
    "#    init_idx[seed_len+ii] = numpy.random.randint(0, len(tokenizer.vocab))\n",
    "\n",
    "for ii in range(max_iter):\n",
    "    kk = numpy.random.randint(0, max_len)\n",
    "    init_idx[seed_len+kk] = tokenizer.convert_tokens_to_ids(['[MASK]'])[0]\n",
    "    out = model(torch.tensor([init_idx]))\n",
    "    if top_k > 0:\n",
    "        logits = out[0,seed_len+kk]\n",
    "        kth_vals, kth_idx = logits.topk(top_k)\n",
    "        dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
    "        init_idx[seed_len+kk] = kth_idx[dist.sample().item()].item()\n",
    "    else:\n",
    "        if ii < burnin:\n",
    "            dist = torch.distributions.categorical.Categorical(logits=out[0,seed_len+kk])\n",
    "            init_idx[seed_len+kk] = dist.sample().item()\n",
    "        else:\n",
    "            init_idx[seed_len+kk] = torch.max(out[0, seed_len+kk],0)[1].item()\n",
    "        \n",
    "    if numpy.mod(ii+1, viz_int) == 0:\n",
    "        for_print = tokenizer.convert_ids_to_tokens(init_idx)\n",
    "        for_print = for_print[:seed_len+kk+1] + ['(*)'] + for_print[seed_len+kk+1:]\n",
    "        print(\"iter\", ii+1, \" \".join(for_print))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
