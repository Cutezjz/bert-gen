{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/08/2019 11:06:27 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz from cache at /home/aw3272/.pytorch_pretrained_bert/7fb0534b83c42daee7d3ddb0ebaa81387925b71665d6ea195c5447f1077454cd.eea60d9ebb03c75bb36302aa9d241d3b7a04bba39c360cf035e8bf8140816233\n",
      "02/08/2019 11:06:27 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /home/aw3272/.pytorch_pretrained_bert/7fb0534b83c42daee7d3ddb0ebaa81387925b71665d6ea195c5447f1077454cd.eea60d9ebb03c75bb36302aa9d241d3b7a04bba39c360cf035e8bf8140816233 to temp dir /state/partition1/job-15143969/tmpgcom6lbj\n",
      "02/08/2019 11:06:37 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "02/08/2019 11:06:48 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "02/08/2019 11:06:48 - INFO - pytorch_pretrained_bert.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt not found in cache, downloading to /state/partition1/job-15143969/tmp445y7u7k\n",
      "100%|██████████| 213450/213450 [00:00<00:00, 9883904.53B/s]\n",
      "02/08/2019 11:06:48 - INFO - pytorch_pretrained_bert.file_utils -   copying /state/partition1/job-15143969/tmp445y7u7k to cache at /home/aw3272/.pytorch_pretrained_bert/cee054f6aafe5e2cf816d2228704e326446785f940f5451a5b26033516a4ac3d.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "02/08/2019 11:06:48 - INFO - pytorch_pretrained_bert.file_utils -   creating metadata file for /home/aw3272/.pytorch_pretrained_bert/cee054f6aafe5e2cf816d2228704e326446785f940f5451a5b26033516a4ac3d.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "02/08/2019 11:06:48 - INFO - pytorch_pretrained_bert.file_utils -   removing temp file /state/partition1/job-15143969/tmp445y7u7k\n",
      "02/08/2019 11:06:48 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt from cache at /home/aw3272/.pytorch_pretrained_bert/cee054f6aafe5e2cf816d2228704e326446785f940f5451a5b26033516a4ac3d.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model_version = 'bert-large-cased'\n",
    "model = BertForMaskedLM.from_pretrained(model_version)\n",
    "model.eval()\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=model_version.endswith(\"uncased\"))\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return [tokenizer.convert_tokens_to_ids(sent) for sent in batch]\n",
    "\n",
    "def untokenize_batch(batch):\n",
    "    return [tokenizer.convert_ids_to_tokens(sent) for sent in batch]\n",
    "\n",
    "def detokenize(sent):\n",
    "    \"\"\" Roughly detokenizes (mainly undoes wordpiece) \"\"\"\n",
    "    new_sent = []\n",
    "    for i, tok in enumerate(sent):\n",
    "        if tok.startswith(\"##\"):\n",
    "            new_sent[len(new_sent) - 1] = new_sent[len(new_sent) - 1] + tok[2:]\n",
    "        else:\n",
    "            new_sent.append(tok)\n",
    "    return new_sent\n",
    "\n",
    "CLS = '[CLS]'\n",
    "SEP = '[SEP]'\n",
    "MASK = '[MASK]'\n",
    "mask_id = tokenizer.convert_tokens_to_ids([MASK])[0]\n",
    "sep_id = tokenizer.convert_tokens_to_ids([MASK])[0]\n",
    "cls_id = tokenizer.convert_tokens_to_ids([MASK])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Generation modes as functions '''\n",
    "\n",
    "def generate_step(out, gen_idx, temperature=None, top_k=0, sample=False):\n",
    "    \"\"\" Generate a word from from out[gen_idx]\n",
    "    \n",
    "    args:\n",
    "        - out (torch.Tensor): tensor of logits of size batch_size x seq_len x vocab_size\n",
    "        - gen_idx (int): location for which to generate for\n",
    "        - top_k (int): if >0, only sample from the top k most probable words\n",
    "        - sample (Bool): if True, sample from full distribution. Overridden by top_k \n",
    "    \"\"\"\n",
    "    logits = out[:, gen_idx]\n",
    "    if temperature is not None:\n",
    "        logits = logits / temperature\n",
    "    if top_k > 0:\n",
    "        kth_vals, kth_idx = logits.topk(top_k, dim=-1)\n",
    "        dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
    "        idx = kth_idx.gather(dim=1, index=dist.sample().unsqueeze(-1)).squeeze(-1).tolist()\n",
    "    elif sample:\n",
    "        dist = torch.distributions.categorical.Categorical(logits=logits)\n",
    "        idx = dist.sample().squeeze(-1).tolist()\n",
    "    else:\n",
    "        idx = torch.argmax(logits, dim=-1).tolist()\n",
    "    return idx\n",
    "\n",
    "def get_init_text(seed_text, max_len, batch_size = 1, rand_init=False):\n",
    "    \"\"\" Get initial sentence by padding seed_text with either masks or random words to max_len \"\"\"\n",
    "    batch = [seed_text + [MASK] * max_len + [SEP] for _ in range(batch_size)]\n",
    "    #if rand_init:\n",
    "    #    for ii in range(max_len):\n",
    "    #        init_idx[seed_len+ii] = np.random.randint(0, len(tokenizer.vocab))\n",
    "    \n",
    "    return tokenize_batch(batch)\n",
    "\n",
    "def parallel_sequential_generation(seed_text, max_len=15, top_k=0, temperature=None, max_iter=300, burnin=200,\n",
    "                                   cuda=False, print_every=10, verbose=True):\n",
    "    \"\"\" Generate for one random position at a timestep\n",
    "    \n",
    "    args:\n",
    "        - burnin: during burn-in period, sample from full distribution; afterwards take argmax\n",
    "    \"\"\"\n",
    "    seed_len = len(seed_text)\n",
    "    batch = get_init_text(seed_text, max_len, batch_size)\n",
    "    \n",
    "    for ii in range(max_iter):\n",
    "        kk = np.random.randint(0, max_len)\n",
    "        for jj in range(batch_size):\n",
    "            batch[jj][seed_len+kk] = mask_id\n",
    "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
    "        out = model(inp)\n",
    "        idxs = generate_step(out, gen_idx=seed_len+kk, top_k=top_k, sample=(ii < burnin))\n",
    "        for jj in range(batch_size):\n",
    "            batch[jj][seed_len+kk] = idxs[jj]\n",
    "            \n",
    "        if verbose and np.mod(ii+1, print_every) == 0:\n",
    "            for_print = tokenizer.convert_ids_to_tokens(batch[0])\n",
    "            for_print = for_print[:seed_len+kk+1] + ['(*)'] + for_print[seed_len+kk+1:]\n",
    "            print(\"iter\", ii+1, \" \".join(for_print))\n",
    "            \n",
    "    return untokenize_batch(batch)\n",
    "\n",
    "def parallel_generation(seed_text, max_len=15, top_k=0, temperature=None, max_iter=300, sample=True, \n",
    "                        cuda=False, print_every=10, verbose=True):\n",
    "    \"\"\" Generate for all positions at a time step \"\"\"\n",
    "    seed_len = len(seed_text)\n",
    "    batch = get_init_text(seed_text, max_len, batch_size)\n",
    "    \n",
    "    for ii in range(max_iter):\n",
    "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
    "        out = model(inp)\n",
    "        for kk in range(max_len):\n",
    "            idxs = generate_step(out, gen_idx=seed_len+kk, top_k=top_k, sample=sample)\n",
    "            for jj in range(batch_size):\n",
    "                batch[jj][seed_len+kk] = idxs[jj]\n",
    "            \n",
    "        if verbose and np.mod(ii, print_every) == 0:\n",
    "            print(\"iter\", ii+1, \" \".join(tokenizer.convert_ids_to_tokens(batch[0])))\n",
    "    \n",
    "    return untokenize_batch(batch)\n",
    "            \n",
    "def sequential_generation(seed_text, batch_size=2, max_len=15, leed_out_len=15, \n",
    "                          top_k=0, temperature=None, sample=True, cuda=False):\n",
    "    \"\"\" Generate one word at a time, in L->R order \"\"\"\n",
    "    seed_len = len(seed_text)\n",
    "    batch = get_init_text(seed_text, max_len, batch_size)\n",
    "    batch = batch.cuda() if cuda else batch\n",
    "    \n",
    "    for ii in range(max_len):\n",
    "        inp = [sent[:seed_len+ii+leed_out_len]+[sep_id] for sent in batch]\n",
    "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
    "        out = model(inp)\n",
    "        idxs = generate_step(out, gen_idx=seed_len+ii, top_k=top_k, sample=sample)\n",
    "        for jj in range(batch_size):\n",
    "            batch[jj][seed_len+ii] = idxs[jj]\n",
    "        \n",
    "        return untokenize_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "def generate(n_samples, seed_text=\"[CLS]\", batch_size=10, max_len=25, \n",
    "             sample=True, top_k=100, temperature=1.0, burnin=200, max_iter=500,\n",
    "             cuda=False, print_every=1):\n",
    "    sentences = []\n",
    "    n_batches = math.ceil(n_samples / batch_size)\n",
    "    start_time = time.time()\n",
    "    for batch_n in range(n_batches):\n",
    "        batch = parallel_sequential_generation(seed_text, max_len=max_len, top_k=top_k,\n",
    "                                               temperature=temperature, burnin=burnin, max_iter=max_iter, \n",
    "                                               cuda=cuda, verbose=False)\n",
    "        \n",
    "        #batch = sequential_generation(seed_text, batch_size=20, max_len=max_len, top_k=top_k, temperature=temperature, leed_out_len=leed_out_len, sample=sample)\n",
    "        #batch = parallel_generation(seed_text, max_len=max_len, top_k=top_k, temperature=temperature, sample=sample, max_iter=max_iter)\n",
    "        \n",
    "        if (batch_n + 1) % print_every == 0:\n",
    "            print(\"Finished batch %d in %.3fs\" % (batch_n + 1, time.time() - start_time))\n",
    "            start_time = time.time()\n",
    "        \n",
    "        sentences += batch\n",
    "    return sentences\n",
    "\n",
    "def printer(sent, should_detokenize=True):\n",
    "    if should_detokenize:\n",
    "        sent = detokenize(sent)\n",
    "    print(\" \".join(sent[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "batch_size = 50\n",
    "max_len = 40\n",
    "top_k = 100\n",
    "temperature= 1.0\n",
    "\n",
    "leed_out_len = 5 # max_len\n",
    "burnin = 200\n",
    "sample = True\n",
    "max_iter = 500\n",
    "\n",
    "# Choose the prefix context\n",
    "seed_text = \"[CLS]\".split()\n",
    "\n",
    "sents = generate(n_samples, seed_text=seed_text, batch_size=batch_size, max_len=max_len,\n",
    "                 sample=sample, top_k=top_k, temperature=temperature, burnin=burnin, max_iter=max_iter,\n",
    "                 cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_file = 'data/generations-len20-burnin200-temp0.700.txt'\n",
    "sents = [detokenize(sent.strip().split()) for sent in open(sent_file).readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for men . men with beards who ploughed up and down . who worked hard\n",
      "towns include the large upper borre valley ( the val de borre ) and borre\n",
      "of the cops are savvy , \" he said . \" they found the murder weapon\n",
      "names , dates , holidays , birthdays . where to next ? \" the response was immediate\n",
      ". hoo . com . \" women in business : a survey and annual report \"\n",
      ", my dad was the one guy who pleaded with him to cut me off all the time\n",
      "video concert 1 - 16 / 2006 . tv video concert 2 - 16 / 2006 . dvd\n",
      "kit , bass , snare , tuba , trombone , cornet , trumpet , french horn\n",
      "flowers are distinctively yellow . subspecies heliopsis hookeri ( golden sunflower ) subsp\n",
      "mother and her new husband filled the house with pillows and blankets and drunken one - night stands\n",
      "which one ? ) esta agua nadie lo que no abrio el ano\n",
      "third - and his final - two books , bone to bone ii , were a national bestseller\n",
      "can see that they have hair and that they wear white clothes with red and black on them\n",
      "the soil , bacteria ( eubacteria ) continue to grow , allowing self - renewal\n",
      ": contractor : telecoms ; information technology . e : contractor : engineering ; technical support services\n",
      "- curated the exhibitions « fotos latinos » 2017 / 18 and 2018 / 19\n",
      "this case , he was also accused of money laundering and bank fraud , under liberian law\n",
      "george v medal , 1914 - 1918 , clasp no . 1 . symons 2001 , p\n",
      "known as picture book ( picture book for children ) . color : red / green / blue\n",
      "( hua guofeng , liu xiaobo ) [ europe / africa / asia ] eq\n",
      "friends , your family , as well the families of \" paolo \" and \" beppe \"\n",
      "functions are denoted by f ( + 1 ) , and are defined by 〈 fₙ 〉\n",
      ": eigenkultur der 2 . wissenschaft des erstes , vol\n",
      "panormus czernyi \" . pappus generosus . british library\n",
      "house , london . penguin classics . penguin books , london . penguin classics . woodcuts\n",
      "; ( former state representatives susana martinez and carlos martinez and senator jorge andrade ) independents\n",
      "is located in the main square that surrounds the city and is known as \" freedom square \"\n",
      ": the national teams of the indian subcontinent compete in an annual tournament organised by bbc world service\n",
      "/ added support for intel x86 ; / / released under gpl v2 . 0\n",
      "8 . \" funeral of duke albert of oldenburg , 30 may 1891 \" . catholic encyclopedia\n",
      "moment that she had been waiting for for a long time , but never with someone like him\n",
      ". a history of philosophy , volumes 1 and 2 : kant and the new materialist philosophy\n",
      ". 15 - orville sells his interest in the griersons to his own law firm\n",
      "of canada : fort laramie was built in 1876 after the battle of sand creek in wyoming\n",
      "/ 88 : name change implemented . it serves as a link between london bridge and london victoria\n",
      ", 2 . ( new york city , ny . ) ( see volumes 1 and 2 )\n",
      "average size of household was 20 males and 20 females : 10 white and 10 african american males\n",
      "had sex with a demon , but not with a half - demon trying to drink from him\n",
      "guy with the camcorder might be there too . ' ' really ? ' ' yeah\n",
      "bosco , australian film . neighbours , australian tv series . last resort , australian tv series\n",
      ". 25 - 1 . 26 lakanan people . they are called as lakanan\n",
      ", like king marduk , was also deeply religious and respected the traditions of the assyrians\n",
      "euston estate also has annexes ( detached houses ) known as \" estate annexes \"\n",
      "is very durable and , when used in place of wood , any imperfections are easily repaired\n",
      "included the discovery of respirator - free oxygen gas , the first known form of oxygen\n",
      "think about it , son . do you want to buy land with no trees or grass ?\n",
      "a lull they recruited drummer micky zito and former buddy holly bassist tony derosa\n",
      "died of arsenic poisoning and was buried in one of cuyahoga valley hot springs\n",
      "album covers were removed and the band opted for self - publishing the liner notes in digital format\n",
      "a ) 50 mhz carrier wave ; ( b ) 50 mhz carrier wave ; ( c )\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    printer(sents[i], should_detokenize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import bleu_score as bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How similar are the generated sentences to the original training data (Toronto Book Corpus and Wikipedia dumps). We follow Yu et al., () and compute the BLEU between the generations and the test sets of both corpora by treating the test set as the references for each generation. The tests sets are large; we subsample 5000 examples from each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_file, replacements={}, uncased=True):\n",
    "    data = [d.strip().split() for d in open(data_file, 'r').readlines()]\n",
    "    if uncased:\n",
    "        data = [[t.lower() for t in sent] for sent in data]\n",
    "        \n",
    "    for k, v in replacements.items():\n",
    "        data = [[t if t != k else v for t in sent] for sent in data]\n",
    " \n",
    "    return data\n",
    "\n",
    "def prepare_wiki(data_file, uncased=True):\n",
    "    replacements = {\"@@unknown@@\": \"[UNK]\"}\n",
    "    return prepare_data(data_file, replacements=replacements, uncased=uncased)\n",
    "\n",
    "def prepare_tbc(data_file):        \n",
    "    replacements = {\"``\": \"\\\"\", \"\\'\\'\": \"\\\"\"}\n",
    "    return prepare_data(data_file, replacements=replacements)\n",
    "\n",
    "def corpus_bleu(generated, references):\n",
    "    \"\"\" Compute similarity between two corpora as measured by\n",
    "    comparing each sentence of `generated` against all sentences in `references` \n",
    "    \n",
    "    args:\n",
    "        - generated (List[List[str]]): list of sentences (split into tokens)\n",
    "        - references (List[List[str]]): list of sentences (split into tokens)\n",
    "        \n",
    "    returns:\n",
    "        - bleu (float)\n",
    "    \"\"\"    \n",
    "    return bleu.corpus_bleu([references for _ in range(len(generated))], generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki103_file = 'data/wiki103.5k.txt'\n",
    "tbc_file = 'data/tbc.5k.txt'\n",
    "\n",
    "wiki_data = prepare_wiki(wiki103_file)\n",
    "tbc_data = prepare_tbc(tbc_file)\n",
    "#sents = [detokenize(sent) for sent in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-577a978460e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BERT-TBC BLEU: %.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcorpus_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtbc_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BERT-Wiki103 BLEU: %.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcorpus_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwiki_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BERT-{TBC + Wiki103} BLEU: %.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcorpus_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtbc_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2500\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwiki_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-6e0042dca49d>\u001b[0m in \u001b[0;36mcorpus_bleu\u001b[0;34m(generated, references)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;34m-\u001b[0m \u001b[0mbleu\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \"\"\"    \n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbleu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreferences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/beegfs/aw3272/software/miniconda3/envs/mtl-sent-rep/lib/python3.6/site-packages/nltk/translate/bleu_score.py\u001b[0m in \u001b[0;36mcorpus_bleu\u001b[0;34m(list_of_references, hypotheses, weights, smoothing_function, auto_reweigh, emulate_multibleu)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;31m# denominator for the corpus-level modified precision.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mp_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodified_precision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0mp_numerators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mp_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mp_denominators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mp_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdenominator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/beegfs/aw3272/software/miniconda3/envs/mtl-sent-rep/lib/python3.6/site-packages/nltk/translate/bleu_score.py\u001b[0m in \u001b[0;36mmodified_precision\u001b[0;34m(references, hypothesis, n)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mngram\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             max_counts[ngram] = max(max_counts.get(ngram, 0),\n\u001b[0;32m--> 300\u001b[0;31m                                     reference_counts[ngram])\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;31m# Assigns the intersection between hypothesis and references' counts.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"BERT-TBC BLEU: %.2f\" % (100 * corpus_bleu(sents, tbc_data)))\n",
    "print(\"BERT-Wiki103 BLEU: %.2f\" % (100 * corpus_bleu(sents, wiki_data)))\n",
    "print(\"BERT-{TBC + Wiki103} BLEU: %.2f\" % (100 * corpus_bleu(sents, tbc_data[:2500] + wiki_data[:2500])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing to existing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OpenAI Generative Pretraining Transformer is another pretrained model successfully used for transfer learning. Since the model is a unidirectional language model, we can straightforwardly generate from the model. See [this repo](https://github.com/huggingface/pytorch-openai-transformer-lm) by Thomas Wolf at Huggingface for instructions for setting up the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, os.path.join(\".\", \"pytorch-openai-transformer-lm\"))\n",
    "\n",
    "from model_pytorch import LMModel, load_openai_pretrained_model, DEFAULT_CONFIG\n",
    "from text_utils import TextEncoder\n",
    "\n",
    "def load_openai_gpt(n_special=1, n_ctx=512):\n",
    "    text_encoder = TextEncoder(\"pytorch-openai-transformer-lm/model/encoder_bpe_40000.json\", \n",
    "                               \"pytorch-openai-transformer-lm/model/vocab_40000.bpe\")\n",
    "    encoder = text_encoder.encoder\n",
    "    n_vocab = len(text_encoder.encoder)\n",
    "    vocab = n_vocab + n_special + n_ctx\n",
    "\n",
    "    args = DEFAULT_CONFIG\n",
    "    lm_model = LMModel(args, vocab, n_ctx, return_probs=True)\n",
    "    load_openai_pretrained_model(lm_model.transformer, n_ctx=n_ctx, n_special=n_special,\n",
    "                                 path=\"pytorch-openai-transformer-lm/model/\",\n",
    "                                 path_names=\"pytorch-openai-transformer-lm/\")\n",
    "    #lm_model.to(device)\n",
    "    lm_model.eval()\n",
    "    return lm_model, text_encoder\n",
    "\n",
    "def make_batch(X, n_vocab, n_special, batch_size):\n",
    "    X = np.array(X)\n",
    "    assert X.ndim in [1, 2]\n",
    "    if X.ndim == 1:\n",
    "        X = np.expand_dims(X, axis=0)\n",
    "    pos_enc = np.arange(n_vocab + n_special, n_vocab + n_special + X.shape[-1])\n",
    "    pos_enc = np.tile(pos_enc, (batch_size, pos_enc.shape[-1])) #np.expand_dims(pos_enc, axis=0)\n",
    "    batch = np.stack([X, pos_enc], axis=-1)\n",
    "    batch = torch.tensor(batch, dtype=torch.long)#.to(device)\n",
    "    return batch\n",
    "\n",
    "def append_batch(X, next_idx):\n",
    "    next_pos = X[:, -1:, 1] + 1\n",
    "    next_x = torch.cat((next_idx, next_pos), -1).unsqueeze(1)\n",
    "    return torch.cat((X, next_x), 1)\n",
    "\n",
    "def generate_sentence_openai(model, text_encoder, seed_text, gen_len=20, topk=100, \n",
    "                             n_vocab=40478, n_special=0, verbose=False):\n",
    "    X = [[n_vocab - 1]]\n",
    "    if seed_text:\n",
    "        seed_ids = text_encoder.encode([seed_text,])\n",
    "        X = [X[0] + seed_ids[0]]\n",
    "        \n",
    "    n_vocab = len(text_encoder.encoder)\n",
    "    XMB = make_batch(X, n_vocab, n_special)\n",
    "    sent = [seed_text]\n",
    "\n",
    "    for _ in range(gen_len):\n",
    "        lm_probs = model(XMB)\n",
    "        if topk == 0:\n",
    "            next_idx = torch.multinomial(lm_probs[:, -1, :n_vocab], 1)\n",
    "        else:\n",
    "            values, indices = lm_probs[:, -1, :n_vocab].topk(topk)\n",
    "            next_idx = indices.gather(-1, torch.multinomial(values, 1))\n",
    "        next_token = next_idx.item()\n",
    "        if next_token == n_vocab:\n",
    "            sent.append(\"<EOS>\")\n",
    "            break\n",
    "        else:\n",
    "            next_token = text_encoder.decoder[next_idx.item()].replace('</w>', '')\n",
    "            sent.append(next_token)\n",
    "            if verbose:\n",
    "                print(next_token, end=' ')\n",
    "        XMB = append_batch(XMB, next_idx)\n",
    "        \n",
    "    return [tok for tok in sent if tok != '\\n']\n",
    "\n",
    "def _generate_sentence_openai(model, text_encoder, seed_text, batch_size=10, gen_len=20, \n",
    "                             topk=100, n_special=0):\n",
    "    n_vocab = len(text_encoder.encoder)\n",
    "    #X = np.random.randint(n_vocab, size=(batch_size, 1)).tolist()\n",
    "    #sents = [[text_encoder.decoder[X[i][0]]].replace('</w>', '') for i in range(batch_size)]\n",
    "    X = [[n_vocab - 1] for _ in range(batch_size)]\n",
    "    sents = [[] for _ in range(batch_size)]\n",
    "    if seed_text:\n",
    "        seed_ids = text_encoder.encode([seed_text,])\n",
    "        X = [X[i] + seed_ids[0] for i in range(batch_size)]\n",
    "        sents = [[seed_text] for _ in range(batch_size)]\n",
    "    XMB = make_batch(X, n_vocab, n_special, batch_size=batch_size)\n",
    "\n",
    "\n",
    "    for _ in range(gen_len):\n",
    "        lm_probs = model(XMB)\n",
    "        if topk == 0:\n",
    "            next_idx = torch.multinomial(lm_probs[:, -1, :], 1)\n",
    "        else:\n",
    "            values, indices = lm_probs[:, -1, :].topk(topk)\n",
    "            next_idx = indices.gather(-1, torch.multinomial(values, 1))\n",
    "        for i in range(batch_size):\n",
    "            next_token = next_idx[i].item()\n",
    "            if next_token == n_vocab:\n",
    "                next_token = \"<EOS>\"\n",
    "            else:\n",
    "                next_token = text_encoder.decoder[next_token].replace('</w>', '')\n",
    "            sents[i].append(next_token)\n",
    "        XMB = append_batch(XMB, next_idx)\n",
    "        \n",
    "    return [[tok for tok in sent if tok != '\\n'] for sent in sents]\n",
    "\n",
    "def generate_openai(model, text_encoder, n_samples, seed_text, \n",
    "                    batch_size=10, gen_len=20, topk=100, \n",
    "                    n_special=0, print_every=1):\n",
    "    sents = []\n",
    "    start_time = time.time()\n",
    "    n_batches = math.ceil(n_samples / batch_size)\n",
    "    for batch_n in range(n_batches):\n",
    "        batch_sents = _generate_sentence_openai(model, text_encoder, seed_text,\n",
    "                                                batch_size=batch_size, gen_len=gen_len, topk=topk, \n",
    "                                                n_special=n_special)\n",
    "        sents += batch_sents\n",
    "        if (batch_n + 1) % print_every == 0:\n",
    "            print(\"Generated batch %d of %d in %.3fs\" % (batch_n + 1, n_batches, time.time() - start_time))\n",
    "            start_time = time.time()\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights...\n"
     ]
    }
   ],
   "source": [
    "gpt_model, gpt_text_encoder = load_openai_gpt(n_special=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated batch 1 of 2 in 30.206s\n",
      "Generated batch 2 of 2 in 29.288s\n"
     ]
    }
   ],
   "source": [
    "openai_sents = generate_openai(gpt_model, gpt_text_encoder, seed_text=\"\", \n",
    "                               n_samples=1000, batch_size=50, gen_len=40,\n",
    "                               n_special=1, print_every=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" no , it 's all right . \" \" you have to , \" he said as he drew her to him . \" you ca n't deny that this child has a special place deep inside where\n",
      "then she looked back at ben 's side . she pointed to the door . \" you think you 're up to these rooms ? \" \" i think i can do it , \" ben said . \"\n",
      "\" it 's a special place , my little friend , \" kate said . they sat for an hour in silence , watching the lights of the city twinkle in the distance across the sky . on\n",
      "\" what was it you wanted to know ? \" i asked as we pulled up in front of the school . \" well , i wanted to know if it was necessary to pick up the other students\n",
      "in desperation , she began to pull her arm away from james , but he would n't let her . \" i love you too much , james , \" she said , \" to do this when you are\n",
      "and now this ? she felt as if she were going back in time and her life was about to turn around to her horror . \" there is no one around here to protect the people we love\n",
      "the door opened and in came one of the men carrying two packs . he approached the bed saying that he 'd brought some of the essentials which were ready . she let them in , thanking him heartily for\n",
      "\" but it is the only safe place . if it was going to be closed , it will be locked up properly . this is n't the safest place to be in this old house . \" \"\n",
      "\" we might meet again , \" i whisper . \" oh , it 'll be nice , \" he says with a laugh . \" i 'll be in touch . \" then the line goes dead\n",
      "\" that 's true , though i 'm not sure about him . he seems very cautious , even as you know . \" \" what 's your next move ? \" a strange sort of tension seemed\n",
      "\" why do you think that would be what my father said ? \" she asked , moving toward the door . \" are you sure you 're not thinking of some sort of escape plan ? \" he\n",
      "\" the other , \" she said , \" is a human 's heart . \" \" jesus , the human heart is alive , \" he said . \" yes . \" the look i gave\n",
      "\" we always leave clues in one place , \" nick said . \" it 's like a computer . maybe we can find out a password when we try it . \" his mind was racing furiously but only\n",
      "his tone and the intensity in his eyes had her shaking all over with nervous excitement . \" and you do n't really want me to f * * k you , do you ? \" he asked ,\n",
      "he turned to her . \" and you were right about the way i handled the situation in st. louis . i had a hard time making up my mind . it seemed like a safe bet for me to\n",
      "\" well , you should certainly go get ready , \" said jason . \" we 'll be there in about thirteen minutes . \" \" did you check the weather forecast ? \" \" unfortunately none was\n",
      "she had no choice . her body had to decide the answer . \" yes . i did . \" her father 's hand squeezed her shoulder . \" you did ? \" max murmured . he was\n",
      "\" i 'm fine , \" she said , placing a hand on his arm . \" really . \" she looked out the window . \" i hate hospitals , and just as i hate all those\n",
      "' yes . ' there was a period of silence before joe said , ' it is n't you you 're brother , is it ? is it me ? ' mary was about to say something when\n",
      "i ca n't make sense of it . it 's all in my head and it comes out as more of a jumble , and then i find the only rational thing i have to say and tell him one\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(\" \".join(openai_sents[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-TBC BLEU: 34.69\n",
      "GPT-Wiki103 BLEU: 9.71\n",
      "GPT-{TBC + Wiki103} BLEU: 30.35\n"
     ]
    }
   ],
   "source": [
    "print(\"GPT-TBC BLEU: %.2f\" % (100 * corpus_bleu(openai_sents, tbc_data)))\n",
    "print(\"GPT-Wiki103 BLEU: %.2f\" % (100 * corpus_bleu(openai_sents, wiki_data)))\n",
    "print(\"GPT-{TBC + Wiki103} BLEU: %.2f\" % (100 * corpus_bleu(openai_sents, tbc_data[:2500] + wiki_data[:2500])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diversity Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-BLEU: treat each sentence as a hypothesis and treat rest of corpus as reference. Lower is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def self_bleu(sents):\n",
    "    return bleu.corpus_bleu([[s for (j, s) in enumerate(sents) if j != i] for i in range(len(sents))], sents)\n",
    "\n",
    "def count_ngrams(max_n=4):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-BLEU: 9.59\n",
      "Self-BLEU: 19.96\n"
     ]
    }
   ],
   "source": [
    "print(\"BERT self-BLEU: %.2f\" % (100 * self_bleu(sents)))\n",
    "print(\"OpenAI self-BLEU: %.2f\" % (100 * self_bleu(openai_sents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Scratch ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Quality measure via outside language models\n",
    "\n",
    "# KN5 (KenLM)\n",
    "# pip install https://github.com/kpu/kenlm/archive/master.zip\n",
    "\n",
    "# Gated Convolutional LM (Fairseq)\n",
    "# https://github.com/pytorch/fairseq/blob/master/examples/language_model/README.md\n",
    "\n",
    "# OpenAI Generative Pretraining LM\n",
    "# https://github.com/huggingface/pytorch-openai-transformer-lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A man of wordly wealth , Sansom was primarily a business man but was also a politician .'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STR = \"A man of wordly wealth, Sansom was primarily a business man but was also a politician.\"\n",
    "\" \".join(detokenize(tokenizer.tokenize(STR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sentence 25 in 60.228s\n",
      "Generated sentence 50 in 57.656s\n",
      "Generated sentence 75 in 57.172s\n",
      "Generated sentence 100 in 57.339s\n",
      "Generated sentence 125 in 58.277s\n",
      "Generated sentence 150 in 58.121s\n",
      "Generated sentence 175 in 58.085s\n",
      "Generated sentence 200 in 57.711s\n",
      "Generated sentence 225 in 57.998s\n",
      "Generated sentence 250 in 57.194s\n",
      "Generated sentence 275 in 57.329s\n",
      "Generated sentence 300 in 58.291s\n",
      "Generated sentence 325 in 57.264s\n",
      "Generated sentence 350 in 57.242s\n",
      "Generated sentence 375 in 57.198s\n",
      "Generated sentence 400 in 57.487s\n",
      "Generated sentence 425 in 62.306s\n",
      "Generated sentence 450 in 57.114s\n",
      "Generated sentence 475 in 57.273s\n",
      "Generated sentence 500 in 57.203s\n",
      "Generated 500 sentences in 482.908m (~57.949s/sentence)\n",
      "Generated sentence 25 in 58.081s\n",
      "Generated sentence 50 in 58.023s\n",
      "Generated sentence 75 in 57.322s\n",
      "Generated sentence 100 in 58.552s\n",
      "Generated sentence 125 in 58.004s\n",
      "Generated sentence 150 in 57.925s\n",
      "Generated sentence 175 in 57.623s\n",
      "Generated sentence 200 in 58.549s\n",
      "Generated sentence 225 in 57.307s\n",
      "Generated sentence 250 in 57.277s\n",
      "Generated sentence 275 in 58.140s\n",
      "Generated sentence 300 in 57.301s\n",
      "Generated sentence 325 in 57.392s\n",
      "Generated sentence 350 in 58.511s\n",
      "Generated sentence 375 in 57.165s\n",
      "Generated sentence 400 in 57.304s\n",
      "Generated sentence 425 in 57.957s\n",
      "Generated sentence 450 in 57.842s\n",
      "Generated sentence 475 in 57.512s\n",
      "Generated sentence 500 in 65.998s\n",
      "Generated 500 sentences in 484.398m (~58.128s/sentence)\n",
      "Generated sentence 25 in 63.436s\n",
      "Generated sentence 50 in 72.187s\n",
      "Generated sentence 75 in 75.261s\n",
      "Generated sentence 100 in 70.779s\n",
      "Generated sentence 125 in 64.371s\n",
      "Generated sentence 150 in 74.569s\n",
      "Generated sentence 175 in 57.940s\n",
      "Generated sentence 200 in 71.494s\n",
      "Generated sentence 225 in 59.047s\n",
      "Generated sentence 250 in 64.907s\n",
      "Generated sentence 275 in 72.105s\n",
      "Generated sentence 300 in 68.935s\n",
      "Generated sentence 325 in 71.085s\n",
      "Generated sentence 350 in 78.970s\n",
      "Generated sentence 375 in 66.321s\n",
      "Generated sentence 400 in 66.812s\n",
      "Generated sentence 425 in 61.138s\n",
      "Generated sentence 450 in 61.238s\n",
      "Generated sentence 475 in 61.369s\n",
      "Generated sentence 500 in 60.276s\n",
      "Generated 500 sentences in 578.444m (~69.413s/sentence)\n",
      "Generated sentence 25 in 62.137s\n",
      "Generated sentence 50 in 65.650s\n",
      "Generated sentence 75 in 67.994s\n",
      "Generated sentence 100 in 61.786s\n",
      "Generated sentence 125 in 60.044s\n",
      "Generated sentence 150 in 60.579s\n",
      "Generated sentence 175 in 61.495s\n",
      "Generated sentence 200 in 63.405s\n",
      "Generated sentence 225 in 62.274s\n",
      "Generated sentence 250 in 62.260s\n",
      "Generated sentence 275 in 62.841s\n",
      "Generated sentence 300 in 59.860s\n",
      "Generated sentence 325 in 65.125s\n",
      "Generated sentence 350 in 60.194s\n",
      "Generated sentence 375 in 61.357s\n",
      "Generated sentence 400 in 66.472s\n",
      "Generated sentence 425 in 61.892s\n",
      "Generated sentence 450 in 62.492s\n",
      "Generated sentence 475 in 63.816s\n",
      "Generated sentence 500 in 59.070s\n",
      "Generated 500 sentences in 522.725m (~62.727s/sentence)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Get some generations \"\"\"\n",
    "import time\n",
    "\n",
    "n_sample = 500\n",
    "max_len = 20\n",
    "top_k = 0\n",
    "temperature = 1.\n",
    "burnin = 200\n",
    "max_iter = 400\n",
    "print_every = 25\n",
    "\n",
    "for top_k in [100]:\n",
    "    for temp in [.1, .5, .7, 2.]:\n",
    "        if top_k:\n",
    "            out_file = \"generations-len%d-topk%d-temp%.3f.txt\" % (max_len, top_k, temp)\n",
    "        else:\n",
    "            out_file = \"generations-len%d-burnin%d-temp%.3f.txt\" % (max_len, burnin, temp)\n",
    "\n",
    "        times = []\n",
    "        with open(out_file, \"w\") as out_fh:\n",
    "            start_time = time.time()\n",
    "            for step_n in range(n_sample):\n",
    "                seed_text = \"[CLS]\".split()\n",
    "                sent = parallel_sequential_generation(seed_text, max_len=max_len, \n",
    "                                                      top_k=top_k, temperature=temp, \n",
    "                                                      burnin=burnin, max_iter=max_iter,\n",
    "                                                      verbose=False)\n",
    "                out_fh.write(\"%s\\n\" % \" \".join(sent[1:-1]))\n",
    "                times.append(time.time() - start_time)\n",
    "                start_time = time.time()\n",
    "                if (step_n + 1) % print_every == 0:\n",
    "                    print(\"Generated sentence %d in %.3fs\" % (step_n + 1, times[-1]))\n",
    "\n",
    "        print(\"Generated %d sentences in %.3fm (~%.3fs/sentence)\" % (n_sample, sum(times) / 60, sum(times) / len(times)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [MASK] york is the greatest city in the world . [SEP] => new ||| rank of new 1\n",
      "[CLS] new [MASK] is the greatest city in the world . [SEP] => york ||| rank of york 1\n",
      "[CLS] new york [MASK] the greatest city in the world . [SEP] => is ||| rank of is 1\n",
      "[CLS] new york is [MASK] greatest city in the world . [SEP] => the ||| rank of the 1\n",
      "[CLS] new york is the [MASK] city in the world . [SEP] => largest ||| rank of greatest 15\n",
      "[CLS] new york is the greatest [MASK] in the world . [SEP] => city ||| rank of city 1\n",
      "[CLS] new york is the greatest city [MASK] the world . [SEP] => in ||| rank of in 1\n",
      "[CLS] new york is the greatest city in [MASK] world . [SEP] => the ||| rank of the 1\n",
      "[CLS] new york is the greatest city in the [MASK] . [SEP] => world ||| rank of world 1\n",
      "[CLS] new york is the greatest city in the world [MASK] [SEP] => . ||| rank of . 1\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "original_sent = [CLS] + 'new york is the greatest city in the world . '.lower().split() + [SEP]\n",
    "\n",
    "for ii_ in range(len(original_sent)-2):\n",
    "    ii = ii_ + 1\n",
    "    new_sent = copy.copy(original_sent)\n",
    "    new_sent[ii] = '[MASK]'\n",
    "#     new_sent[ii] = tokenizer.convert_ids_to_tokens([numpy.random.randint(0, len(tokenizer.vocab))])[0]\n",
    "    out = model(torch.tensor([tokenizer.convert_tokens_to_ids(new_sent)]))\n",
    "    pred = tokenizer.convert_ids_to_tokens([out[0][ii].max(0)[1].item()])[0]\n",
    "    probs = out[0][ii].data.numpy()\n",
    "    rank = len(tokenizer.vocab) - numpy.argsort(numpy.argsort(probs))[tokenizer.convert_tokens_to_ids([original_sent[ii]])[0]]\n",
    "    print(\" \".join(new_sent), \"=>\", pred, '|||', 'rank of', original_sent[ii], rank)\n",
    "#     if pred == 'the':\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" . . . . . , . . , . . . . . , . . . . [MASK]\n",
      "\" and Felix - ( - ) = + . - = . he = . - = . . [MASK]\n",
      "the and formula ##s ; , and - algebra ; ; . . . . . ; . . . [MASK]\n",
      "/ was . by . . . . . . and , from , gave . . . . . [MASK]\n",
      "* by army use of and as the of were applied as ( ( ) , and = ) . [MASK]\n",
      ". and ##i . . , . . . and ... , . . , to the part , . [MASK]\n",
      ". you ; ; ; ; ; ' Mr . Scott to the and ##ra of the . \" . [MASK]\n",
      ". . ##1 . : and . . . : . . : . . . . . . . [MASK]\n",
      "king - as of 2014 . | . / _ . / < < - | . / > | [MASK]\n",
      ". ##2 = . . = = = = - . = = = = ( = = ) | [MASK]\n"
     ]
    }
   ],
   "source": [
    "''' sequential generation: this one kinda works '''\n",
    "\n",
    "\n",
    "sep_id = tokenizer.convert_tokens_to_ids([SEP])\n",
    "sample = True\n",
    "max_len = 20\n",
    "leed_out_len = 5 #max_len\n",
    "random_future = False\n",
    "top_k = 100 # set it to 0 if you don't want top_k\n",
    "n_samples = 1\n",
    "\n",
    "seed_text = [[CLS] for _ in range(batch_size)]\n",
    "seed_len = len(seed_text[0])\n",
    "\n",
    "for si in range(n_samples):\n",
    "    #init_text = seed_text + ['[MASK]'] * max_len\n",
    "    init_text = [seed + ['[MASK]'] * max_len for seed in seed_text]\n",
    "    init_idx = tokenize_batch(init_text) #tokenizer.convert_tokens_to_ids(init_text)\n",
    "    #if random_future:\n",
    "    #    for ii in range(max_len):\n",
    "    #        init_idx[seed_len+ii] = numpy.random.randint(0, len(tokenizer.vocab))\n",
    "\n",
    "    for ii in range(max_len):\n",
    "        out = model(torch.tensor([i[:seed_len+ii+leed_out_len]+sep_id for i in init_idx]))\n",
    "        if top_k > 0:\n",
    "            logits = out[:,seed_len+ii]\n",
    "            kth_vals, kth_idx = logits.topk(top_k, dim=1)\n",
    "            dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
    "            new_idxs = kth_idx.gather(dim=1, index=dist.sample().unsqueeze(-1)).squeeze(-1).tolist()\n",
    "            for jj in range(len(init_idx)):\n",
    "                init_idx[jj][ii] = new_idxs[jj]\n",
    "        else:\n",
    "            if sample:\n",
    "                dist = torch.distributions.categorical.Categorical(logits=out[0,seed_len+ii])\n",
    "                init_idx[seed_len+ii] = dist.sample().item()\n",
    "            else:\n",
    "                init_idx[seed_len+ii] = torch.max(out[0, seed_len+ii],0)[1].item()\n",
    "\n",
    "#     print(init_idx)\n",
    "    for sent in init_idx:\n",
    "        print(\" \".join(tokenizer.convert_ids_to_tokens(sent)))\n",
    "# print(\" \".join(tokenizer.convert_ids_to_tokens(init_idx)).replace(\" ##\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[119], [119], [1103], [170], [168], [1110], [119], [176], [119], [119]]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1 [CLS] philippine \" ##hara ##id on mir by character sons five god with the , ; for a fatal ##in ; [SEP]\n",
      "iter 11 [CLS] 2 m be ##h on ##r by the to . god with . aid ; for present definite h . [SEP]\n",
      "iter 21 [CLS] 2 ##m be ##h on ##r by the to . god with . aid ; for present definite h . [SEP]\n",
      "iter 31 [CLS] 2 ##m be ##h on ##s by the to . god with . aid ; for present definite h . [SEP]\n",
      "iter 41 [CLS] 2 ##m be ##h on ze by the to . god with . aid ; for which definite h . [SEP]\n",
      "iter 51 [CLS] 2 ##m be ##h on - by the to . god with . aid ; p or an h . [SEP]\n",
      "iter 61 [CLS] 2 ##m be ##h on made by the to . god with . help the p or an h . [SEP]\n",
      "iter 71 [CLS] 2 ##b be ##h on made by the to . god with . help the p or an h . [SEP]\n",
      "iter 81 [CLS] 2 ##b be ##h on made by the to . god with . help the p or an h . [SEP]\n",
      "iter 91 [CLS] 2 ##b be ##h is made by the to . god with . help the p or an h . [SEP]\n"
     ]
    }
   ],
   "source": [
    "''' parallel generation: this one doesn't work '''\n",
    "\n",
    "sample = True\n",
    "max_iter = 100\n",
    "viz_int = 10\n",
    "max_len = 20\n",
    "top_k = 0\n",
    "\n",
    "seed_text = '[CLS]'.split()\n",
    "seed_len = len(seed_text)\n",
    "\n",
    "init_text = seed_text + ['[MASK]'] * max_len + ['[SEP]']\n",
    "init_idx = tokenizer.convert_tokens_to_ids(init_text)\n",
    "# for ii in range(max_len):\n",
    "#     init_idx[seed_len+ii] = numpy.random.randint(0, len(tokenizer.vocab))\n",
    "\n",
    "for ii in range(max_iter):\n",
    "    out = model(torch.tensor([init_idx]))\n",
    "    for kk in range(max_len):\n",
    "        if top_k > 0:\n",
    "            logits = out[0,seed_len+kk]\n",
    "            kth_vals, kth_idx = logits.topk(top_k)\n",
    "            dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
    "            init_idx[seed_len+kk] = kth_idx[dist.sample().item()].item()\n",
    "        else:\n",
    "            if sample:\n",
    "                dist = torch.distributions.categorical.Categorical(logits=out[0,seed_len+kk])\n",
    "                init_idx[seed_len+kk] = dist.sample().item()\n",
    "            else:\n",
    "                init_idx[seed_len+kk] = torch.max(out[0, seed_len+kk],0)[1].item()\n",
    "    if numpy.mod(ii, viz_int) == 0:\n",
    "        print(\"iter\", ii+1, \" \".join(tokenizer.convert_ids_to_tokens(init_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10 [CLS] un (*) ##i [MASK] [MASK] [MASK] ; [MASK] . [MASK] ##i [MASK] : ; [MASK] [MASK] [SEP]\n",
      "iter 20 [CLS] xx ##ix [MASK] ; [MASK] . [MASK] . (*) [MASK] . [MASK] 2 ; b [MASK] [SEP]\n",
      "iter 30 [CLS] vi (*) . [MASK] ; [MASK] . [MASK] . 17 . § 2 : ii . [SEP]\n",
      "iter 40 [CLS] iii . 11 ; norway . iii . 17 . § 87 (*) . 1 . [SEP]\n",
      "iter 50 [CLS] iii . (*) sweden . norway § 11 . 17 & § 87 . 12 . [SEP]\n",
      "iter 60 [CLS] iii . denmark (*) & norway § 11 . 17 . § 87 . 20 . [SEP]\n",
      "iter 70 [CLS] iii . denmark & norway § 87 . 17 ; § 87 . (*) 20 ; [SEP]\n",
      "iter 80 [CLS] 4 . denmark & norway § 87 (*) . 17 ; § 87 . 20 ; [SEP]\n",
      "iter 90 [CLS] 4 . denmark & sweden § (*) 85 . 11 ; § 87 . 20 ; [SEP]\n",
      "iter 100 [CLS] 4 - denmark & norway (*) § 86 . 6 ; § 86 . 2 ; [SEP]\n",
      "iter 110 [CLS] cf . denmark - (*) norway § 86 . 5 ; § 86 . 7 ; [SEP]\n",
      "iter 120 [CLS] cf . denmark - schleswig (*) § 1886 . 5 , § 86 . 1 ; [SEP]\n",
      "iter 130 [CLS] cf . denmark v schleswig § 86 . (*) 5 , § 86 . 1 ; [SEP]\n",
      "iter 140 [CLS] cf . (*) denmark ser . § 86 . 2 , § 86 . 6 ; [SEP]\n",
      "iter 150 [CLS] cf . denmark ser . § 86 . 0 , § 86 . 2 ; (*) [SEP]\n",
      "iter 160 [CLS] em . denmark 56 (*) . § 86 . 0 ; § 86 . 45 ; [SEP]\n",
      "iter 170 [CLS] 1 (*) . denmark 56 . § 86 . 42 ; § 86 . 45 ; [SEP]\n",
      "iter 180 [CLS] 5 . ch 56 . § 86 . 42 , § 86 (*) . 45 . [SEP]\n",
      "iter 190 [CLS] 5 . ch ##s . § 86 . 35 , § (*) 86 . 45 . [SEP]\n",
      "iter 200 [CLS] 5 . pre ##s . § 86 . 35 and § 86 . 45 . (*) [SEP]\n",
      "iter 210 [CLS] 5 . con ##s . § 86 . 44 . (*) § 86 . 45 . [SEP]\n",
      "iter 220 [CLS] 5 . con ##st . § (*) 86 . 44 . § 86 . 45 . [SEP]\n",
      "iter 230 [CLS] 5 . di ##st . § (*) 86 . 44 , § 86 . 45 . [SEP]\n",
      "iter 240 [CLS] 5 . di ##st . § 86 . 44 , § (*) 86 . 45 . [SEP]\n",
      "iter 250 [CLS] stat . di ##st . § 86 . 44 (*) . § 86 . 45 . [SEP]\n",
      "iter 260 [CLS] stat . di ##st . § 86 . 44 . § 86 . 45 . (*) [SEP]\n",
      "iter 270 [CLS] stat . di ##st . § (*) 86 . 44 . § 86 . 45 . [SEP]\n",
      "iter 280 [CLS] stat . di ##st . § 86 (*) . 44 . § 86 . 45 . [SEP]\n",
      "iter 290 [CLS] stat . di ##st . § 86 . (*) 44 . § 86 . 45 . [SEP]\n",
      "iter 300 [CLS] stat (*) . di ##st . § 86 . 44 . § 86 . 45 . [SEP]\n"
     ]
    }
   ],
   "source": [
    "''' parallel-sequential generation: this one definitely works '''\n",
    "\n",
    "# sample = True\n",
    "burnin = 200\n",
    "max_iter = 300\n",
    "viz_int = 10\n",
    "max_len = 15\n",
    "top_k = 0\n",
    "\n",
    "seed_text = '[CLS]'.split()\n",
    "seed_len = len(seed_text)\n",
    "\n",
    "init_text = seed_text + ['[MASK]'] * (max_len) + ['[SEP]']\n",
    "init_idx = tokenizer.convert_tokens_to_ids(init_text)\n",
    "#for ii in range(max_len):\n",
    "#    init_idx[seed_len+ii] = numpy.random.randint(0, len(tokenizer.vocab))\n",
    "\n",
    "for ii in range(max_iter):\n",
    "    kk = numpy.random.randint(0, max_len)\n",
    "    init_idx[seed_len+kk] = tokenizer.convert_tokens_to_ids(['[MASK]'])[0]\n",
    "    out = model(torch.tensor([init_idx]))\n",
    "    if top_k > 0:\n",
    "        logits = out[0,seed_len+kk]\n",
    "        kth_vals, kth_idx = logits.topk(top_k)\n",
    "        dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
    "        init_idx[seed_len+kk] = kth_idx[dist.sample().item()].item()\n",
    "    else:\n",
    "        if ii < burnin:\n",
    "            dist = torch.distributions.categorical.Categorical(logits=out[0,seed_len+kk])\n",
    "            init_idx[seed_len+kk] = dist.sample().item()\n",
    "        else:\n",
    "            init_idx[seed_len+kk] = torch.max(out[0, seed_len+kk],0)[1].item()\n",
    "        \n",
    "    if numpy.mod(ii+1, viz_int) == 0:\n",
    "        for_print = tokenizer.convert_ids_to_tokens(init_idx)\n",
    "        for_print = for_print[:seed_len+kk+1] + ['(*)'] + for_print[seed_len+kk+1:]\n",
    "        print(\"iter\", ii+1, \" \".join(for_print))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mtl-sent-rep]",
   "language": "python",
   "name": "conda-env-mtl-sent-rep-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
